{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs2Yk5y3IxKb"
   },
   "source": [
    "Deep artificial neural network for expressive timing predictions in musical pieces\n",
    "---------------\n",
    "\n",
    "This notebook loads the data generated from the note level processing notebook and uses them to train and test a long sequence-based artificial neural network for predicting the onset timing deviation of notes from the MusicNet dataset pieces.\n",
    "\n",
    "\n",
    "#### Load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5sQCWWMtIxKg",
    "outputId": "c0937385-1926-4992-ccd9-6dba03c3e71a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#  read dataset\n",
    "runLocal = True\n",
    "if runLocal:\n",
    "    pathRoot = 'data/'\n",
    "else:\n",
    "    pathRoot = '/content/drive/My Drive/colab_data/'\n",
    "\n",
    "with open(pathRoot + 'note_sequences.data', 'rb') as seq_path:\n",
    "    sequences = pickle.load(seq_path)\n",
    "with open(pathRoot + 'note_sequences_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "    ix_to_lex = {v: k for k, v in lex_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset: Beethoven sonatas\n",
    "with open('/Users/fabiojmortega/musicnet/LvB_violinSonatas.json', 'r') as f:\n",
    "        txt = f.read()\n",
    "        subset = eval(txt)\n",
    "subseq = []\n",
    "svals = [y for x in subset.values() for y in x]\n",
    "for (s, sid) in sequences:\n",
    "    if sid in svals:\n",
    "        subseq.append((s, sid))\n",
    "sequences = subseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzL3Y9MOIxLG"
   },
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "def my_model(tx, ty, n_x, n_y, vocab_col, vocab_size):\n",
    "    \n",
    "    hidden_size = 64\n",
    "    reg_lambda = 0.01\n",
    "    \n",
    "    X = Input((tx, n_x))\n",
    "    \n",
    "    #  Split the input vector between one-hot and numerical features\n",
    "    mk = list(range(n_x))\n",
    "    mk.remove(vocab_col)\n",
    "    \n",
    "    emb_input = layers.Lambda(lambda x: x[:, :, vocab_col])(X)\n",
    "    num_input = layers.Lambda(lambda x: tf.gather(x, mk, axis=2))(X)\n",
    "\n",
    "    #  Compute an embedding vector and combine it with the numeric features\n",
    "    emb_mel = layers.Embedding(input_dim=vocab_size, output_dim=hidden_size, mask_zero=True,\n",
    "                               embeddings_regularizer=tf.keras.regularizers.l2(reg_lambda))(emb_input)\n",
    "    emb_rest = layers.Dense(hidden_size, activation='relu',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                            bias_regularizer=tf.keras.regularizers.l2(reg_lambda))(num_input)\n",
    "    \n",
    "    seq_input = layers.Concatenate(axis=2)([emb_mel, emb_rest])\n",
    "    seq_input = layers.BatchNormalization()(seq_input)\n",
    "    \n",
    "    #  Run a sequence model\n",
    "    tensor_var = layers.Bidirectional(layers.LSTM(hidden_size * 2, return_sequences = True,\n",
    "                                                  kernel_regularizer=tf.keras.regularizers.l2(reg_lambda), \n",
    "                                                  bias_regularizer=tf.keras.regularizers.l2(reg_lambda)))(seq_input)\n",
    "    tensor_var = layers.Dropout(0.2)(tensor_var)\n",
    "    tensor_var = layers.BatchNormalization()(tensor_var)\n",
    "    tensor_var = layers.LSTM(hidden_size, return_sequences = True, \n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(reg_lambda))(tensor_var)\n",
    "    tensor_var = layers.Dropout(0.2)(tensor_var)\n",
    "    tensor_var = layers.BatchNormalization()(tensor_var)    \n",
    "    \n",
    "    #  if not 1-to-1, fully-connected layer across time to generate outputs\n",
    "    if ty < tx:\n",
    "        tensor_var = layers.Dense(ty, activation='relu')(tensor_var)\n",
    "        Y = layers.Dense(n_y)(tensor_var)\n",
    "    else:\n",
    "        Y = layers.TimeDistributed(layers.Dense(n_y, kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                                bias_regularizer=tf.keras.regularizers.l2(reg_lambda)))(tensor_var)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "sOFtw2tuIxLJ",
    "outputId": "9a8f9a85-d095-4782-e5ac-b9e654083ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 16)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 200)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 200, 15)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 64)      53184       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200, 64)      1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200, 128)     0           embedding[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200, 128)     512         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 200, 256)     263168      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 200, 256)     0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 256)     1024        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 200, 64)      82176       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200, 64)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 64)      256         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 200, 1)       65          batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 401,409\n",
      "Trainable params: 400,513\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, sequence_length, batch_size=64, sequence_stride=1,\n",
    "                 shuffle=None, fit=True, output_sequence=True, output_cols=None,\n",
    "                 mini_batch_limit=np.inf):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.shuffle = fit if shuffle is None else shuffle\n",
    "        self.fit = fit\n",
    "        self.output_sequence = output_sequence\n",
    "        self.pad_value = 0.\n",
    "        self.mini_batch_limit = mini_batch_limit\n",
    "        self.indexes = []\n",
    "        if output_cols is None:\n",
    "            self.output_cols = data[0][1].columns\n",
    "        else:\n",
    "            self.output_cols = output_cols\n",
    "        for si, s in enumerate(data):\n",
    "            x = s[0]\n",
    "            tx = x.shape[0]\n",
    "            xind = 0\n",
    "            while tx > sequence_length:\n",
    "                self.indexes.append((si, xind))\n",
    "                xind += sequence_stride\n",
    "                tx -= sequence_stride\n",
    "            self.indexes.append((si,xind))\n",
    "        np.random.shuffle(self.indexes)  # always shuffle once\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.min([len(self.indexes) / self.batch_size, self.mini_batch_limit]))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.batch_size\n",
    "        this_size = self.batch_size if index + self.batch_size < len(self.indexes) else len(self.indexes) - index\n",
    "        X = np.zeros((this_size, self.sequence_length, self.data[0][0].shape[1]))\n",
    "        Y = np.zeros((this_size, self.sequence_length, len(self.output_cols)))\n",
    "        for i in range(this_size):\n",
    "            X[i,:,:], Y[i,:,:] = self.__getsingleitem(index + i)\n",
    "        if self.fit:\n",
    "            if self.output_sequence:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y[:,-1,:]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def __getsingleitem(self, index):\n",
    "        (seq, stride) = self.indexes[index]\n",
    "        (X, Y, _, _, _) = self.data[seq]\n",
    "        Y = Y.loc[:, self.output_cols]\n",
    "        if stride+self.sequence_length <= X.shape[0]:\n",
    "            if self.fit:\n",
    "                X = X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "                if self.output_sequence:\n",
    "                    Y = Y.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "                else:\n",
    "                    Y = Y.iloc[stride+self.sequence_length-1, :].to_numpy(dtype='float64').reshape((1,len(self.output_cols)))\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "        else:\n",
    "            # pad\n",
    "            X = X.iloc[stride:X.shape[0], :].to_numpy(dtype='float64')\n",
    "            padX = np.full((self.sequence_length - X.shape[0], X.shape[1]), self.pad_value)\n",
    "            if self.fit:\n",
    "                if self.output_sequence:\n",
    "                    Y = Y.iloc[stride:Y.shape[0], :].to_numpy(dtype='float64')\n",
    "                    padY = np.full((self.sequence_length - Y.shape[0], Y.shape[1]), self.pad_value)\n",
    "                    return np.concatenate((X, padX), axis=0), np.concatenate((Y, padY), axis=0)\n",
    "                else:\n",
    "                    padY = np.full((1, Y.shape[1]), self.pad_value)\n",
    "            else:\n",
    "                return np.concatenate((X, padX), axis=0).reshape((1, self.sequence_length, X.shape[1]))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "np.random.seed(1728)\n",
    "seq_length = 200\n",
    "output_cols = ['ioiRatio']\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "for (s, sid) in sequences:\n",
    "    if sid in subset[2]:\n",
    "        for (x, y, tr, i, mm) in s:\n",
    "            if tr == 0: #  no transposition\n",
    "                val += [(x, y, sid, i, mm)]\n",
    "    else:\n",
    "        train += s\n",
    "\n",
    "# np.random.shuffle(sequences)  # shuffle before splitting validation set\n",
    "# val_split_ix = int(0.9*len(sequences))\n",
    "# train = []\n",
    "# val = []\n",
    "# for (s, _) in sequences[:val_split_ix]:\n",
    "#     train += s\n",
    "# for (sv, p) in sequences[val_split_ix:]:\n",
    "#     for (x, y, tr, i, mm) in sv:\n",
    "#         if tr == 0: #  no transposition\n",
    "#             val += [(x, y, p, i, mm)]\n",
    "\n",
    "# sequences = None  # if you need a bit more memory, to allow garbage collection\n",
    "\n",
    "# uncomment to reduce ds for testing\n",
    "# train = train[:10]\n",
    "# val = val[:10]\n",
    "\n",
    "generator = DataGenerator(train, seq_length, output_sequence=True, output_cols=output_cols)\n",
    "                          #shuffle=False, mini_batch_limit=50)\n",
    "val_gen = DataGenerator(val, seq_length, output_sequence=True, output_cols=output_cols)\n",
    "                        #shuffle=False, mini_batch_limit=25)\n",
    "\n",
    "model = my_model(seq_length, seq_length, train[0][0].shape[1], len(output_cols),\n",
    "                 train[0][0].columns.get_loc(\"melody\"), len(ix_to_lex) + 3)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB1zXiP9IxLV"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "JpghmUE3IxLX",
    "outputId": "6c2e7186-6bf7-41ac-ea28-387da0e9d393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2111/2111 [==============================] - 4630s 2s/step - loss: 1.4323 - val_loss: 1.1470\n",
      "Epoch 2/2\n",
      "2111/2111 [==============================] - 3763s 2s/step - loss: 0.3418 - val_loss: 1.2385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11ac27190>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generator, epochs=2, validation_data=val_gen)\n",
    "\n",
    "#  Save model\n",
    "# model.save_weights(pathRoot + '2020-08-31_timing.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mv-wgSVTIxLj"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2eV2YV0IxL1",
    "outputId": "2af81d2a-bd9b-47a8-a0d3-038baaf6e948"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "# model.load_weights(pathRoot + '2020-09-03_huge.h5')\n",
    "\n",
    "#  Compute note-level error\n",
    "\n",
    "# validation data\n",
    "test_sequences = val\n",
    "\n",
    "# test data\n",
    "# with open(pathRoot + 'note_sequences_test.data', 'rb') as seq_path:\n",
    "#     test_sequences = pickle.load(seq_path)\n",
    "#     ts = []\n",
    "#     for (sv, p) in test_sequences:\n",
    "#         for (x, y, tr, i, mm) in sv:\n",
    "#             if tr == 0:\n",
    "#                 ts.append((x, y, p, i, mm))\n",
    "#     test_sequences = ts\n",
    "\n",
    "def evaluation(sequences, sequence_length, model, pad_value=0.):\n",
    "    Yhat = []\n",
    "    for S in sequences:\n",
    "        X = S[0]\n",
    "        tx = X.shape[0]\n",
    "        n_x = int(tx / sequence_length)\n",
    "        n_x += 0 if tx % sequence_length == 0 else 1\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        for i in range(n_x - 1):            \n",
    "            x[i,:,:] = X.iloc[(i * sequence_length):(i + 1) * sequence_length,:].to_numpy()\n",
    "        x[n_x - 1,:tx - (n_x - 1) * sequence_length,:] = X.iloc[(n_x - 1) * sequence_length:,:].to_numpy()\n",
    "        y = model.predict(x)\n",
    "        Yhat.append(y.reshape((-1,y.shape[2])))\n",
    "    return Yhat\n",
    "\n",
    "def sliding_evaluation(sequences, sequence_length, model, pad_value=0., pad_start=True, window_size=None):\n",
    "    Yhat = []\n",
    "    if window_size is None:\n",
    "        margin = int(0.1 * sequence_length)\n",
    "        window_size = int(sequence_length - 2*margin)\n",
    "    else:\n",
    "        margin = int((sequence_length - window_size) / 2)\n",
    "    for S in sequences:\n",
    "        X = S[0]\n",
    "        tx = X.shape[0]\n",
    "        n_x = int(np.ceil(tx / window_size) if pad_start else np.ceil((tx - margin) / window_size))\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        if pad_start:\n",
    "            idx = window_size\n",
    "            x[0,margin:,:] = X.iloc[0:sequence_length-margin,:].to_numpy()\n",
    "        else:\n",
    "            idx = margin + window_size\n",
    "            x[0,:,:] = X.iloc[0:sequence_length,:].to_numpy()\n",
    "        for i in range(1, n_x):\n",
    "            if idx + sequence_length <= tx:\n",
    "                x[i,:,:] = X.iloc[idx:idx+sequence_length,:].to_numpy()\n",
    "            else:\n",
    "                x[i,:tx - idx,:] = X.iloc[idx:,:].to_numpy()\n",
    "            idx += window_size\n",
    "        y = model.predict(x)\n",
    "        if pad_start:\n",
    "            Yhat.append(y[:,margin:-margin,:].reshape(-1, y.shape[-1]))\n",
    "        else:\n",
    "            Yhat.append(np.concatenate((y[0,:-margin,:], y[1:,margin:-margin,:].reshape(-1, y.shape[-1]))))\n",
    "    return Yhat\n",
    "\n",
    "Yhat = evaluation(test_sequences, seq_length, model)\n",
    "mse = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "ms = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "for i, (_, Y, _, _, _) in enumerate(test_sequences):\n",
    "    Y = Y.loc[:,output_cols]\n",
    "    mse[i,:] = np.mean((Yhat[i][:Y.shape[0],:] - Y) ** 2)\n",
    "    ms[i,:] = np.mean(Y ** 2)\n",
    "    \n",
    "print('Validation set MSE for y_0: ' + str(np.mean(mse[:,0])) + '     mean square val: ' + str(np.mean(ms[:,0])))\n",
    "print('Minimum y_0 MSE among pieces: ' + str(mse[:,0].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mse[:,0])\n",
    "plt.plot(ms[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "piece = 5\n",
    "attr = ['ioiRatio']\n",
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(Yhat[piece][:,0])\n",
    "plt.plot(test_sequences[piece][1].loc[:,attr].to_numpy())\n",
    "# print(test_sequences[piece][1].columns[attr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listen to a synthesized predicted expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import IPython.display\n",
    "\n",
    "test_sequences = val\n",
    "\n",
    "# piece to synthesize:\n",
    "pieceNum = 27\n",
    "pieceId = test_sequences[pieceNum][2]\n",
    "print(pieceId)\n",
    "\n",
    "deviations_pred = Yhat[pieceNum][:,0] * test_sequences[pieceNum][4][2,1] + test_sequences[pieceNum][4][2,0]\n",
    "deviations_perf = test_sequences[pieceNum][1].ioiRatio * test_sequences[pieceNum][4][2,1] + test_sequences[pieceNum][4][2,0]\n",
    "tempo = test_sequences[pieceNum][1].localTempo.iloc[0] * test_sequences[pieceNum][4][0,1] + test_sequences[pieceNum][4][0,0]\n",
    "no_dev = [test_sequences[pieceNum][4][2,0]] * test_sequences[pieceNum][1].shape[0]\n",
    "dev_rand = np.random.normal(size=test_sequences[pieceNum][1].shape[0]) * test_sequences[pieceNum][4][2,1] + test_sequences[pieceNum][4][2,0]\n",
    "pm = pretty_midi.PrettyMIDI(initial_tempo=60 * tempo)\n",
    "inst = pretty_midi.Instrument(program=test_sequences[pieceNum][3], is_drum=False, name='melody_inst')\n",
    "pm.instruments.append(inst)\n",
    "start = 0.\n",
    "lastNote = None\n",
    "for x, y, dev in zip(test_sequences[pieceNum][0].itertuples(), test_sequences[pieceNum][1].itertuples(), deviations_perf):\n",
    "    (pitch, _) = ix_to_lex[x.melody]\n",
    "    if lastNote:\n",
    "        if start < lastNote.end:\n",
    "            lastNote.end = start\n",
    "    end = start + (x.duration * moments['duration'][1] + moments['duration'][0]) * dev\n",
    "    lastNote = pretty_midi.Note(100, pitch, start, end)\n",
    "    inst.notes.append(lastNote)\n",
    "    start += (x.ioi * moments['ioi'][1] + moments['ioi'][0]) * dev\n",
    "IPython.display.Audio(pm.fluidsynth(fs=44100), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building conductive input from generated performance\n",
    "\n",
    "This step uses the predicted timing information to build a local tempo signal which can be used as input in a virtual conductor. That signal is compared to the local tempo vector obtained from the chosen reference performance from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PhraseDynamicsLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
