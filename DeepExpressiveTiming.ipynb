{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs2Yk5y3IxKb"
   },
   "source": [
    "Deep artificial neural network for expressive timing predictions in musical pieces\n",
    "---------------\n",
    "\n",
    "This notebook loads the data generated from the note level processing notebook and uses them to train and test a long sequence-based artificial neural network for predicting the onset timing deviation of notes from the MusicNet dataset pieces.\n",
    "\n",
    "\n",
    "#### Load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5sQCWWMtIxKg",
    "outputId": "c0937385-1926-4992-ccd9-6dba03c3e71a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#  read dataset\n",
    "runLocal = True\n",
    "if runLocal:\n",
    "    pathRoot = 'data/'\n",
    "else:\n",
    "    pathRoot = '/content/drive/My Drive/colab_data/'\n",
    "\n",
    "with open(pathRoot + 'note_sequences.data', 'rb') as seq_path:\n",
    "    sequences = pickle.load(seq_path)\n",
    "with open(pathRoot + 'note_sequences_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "    ix_to_lex = {v: k for k, v in lex_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute startTime first-order difference as output feature\n",
    "\n",
    "with open(pathRoot + 'normalizer.data', 'rb') as filehandle:\n",
    "    moments, cols = pickle.load(filehandle)\n",
    "    moments = dict(zip(cols, list(moments)))\n",
    "\n",
    "for ind, (s, sid) in enumerate(sequences):\n",
    "    for i in range(len(s)):\n",
    "        (x, y, tr, inst, mm) = s[i]\n",
    "        ioiS = y.startTime.diff().to_numpy() * moments['startTime'][1]\n",
    "        ioi = x.ioi.to_numpy() * moments['ioi'][1] + moments['ioi'][0]\n",
    "        last = (y.durationSecs.iloc[-1] * moments['durationSecs'][1] + moments['durationSecs'][0]) / ioi[-1]\n",
    "        ioiRatio = np.concatenate((ioiS[1:] / ioi[:-1], [last]))\n",
    "        if np.any(np.isnan(ioiRatio)):\n",
    "            print(\"nan: {a}, {b}, {c}, {d}\".format(a=np.argwhere(np.isnan(ioiRatio)), b=ind, c=tr, d=inst))\n",
    "        mm2 = np.zeros((mm.shape[0]+1, 2))\n",
    "        mm2[0:2,:] = mm\n",
    "        mm2[2,0] = ioiRatio.mean()\n",
    "        mm2[2,1] = ioiRatio.std()\n",
    "        y['ioiRatio'] = (ioiRatio - mm2[2,0]) / mm2[2,1]\n",
    "        s[i] = (x, y, tr, inst, mm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzL3Y9MOIxLG"
   },
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "def my_model(tx, ty, n_x, n_y, vocab_col, vocab_size):\n",
    "    X = Input((tx, n_x))\n",
    "    \n",
    "    #  Split the input vector between one-hot and numerical features\n",
    "    mk = list(range(n_x))\n",
    "    mk.remove(vocab_col)\n",
    "    \n",
    "    emb_input = layers.Lambda(lambda x: x[:, :, vocab_col])(X)\n",
    "    num_input = layers.Lambda(lambda x: tf.gather(x, mk, axis=2))(X)\n",
    "\n",
    "    #  Compute an embedding vector and combine it with the numeric features\n",
    "    emb_vec = layers.Embedding(input_dim=vocab_size, output_dim=64, mask_zero=True)(emb_input)\n",
    "    seq_input = layers.Concatenate(axis=2)([emb_vec, num_input])\n",
    "    \n",
    "    #  Run a sequence model\n",
    "    tensor_var = layers.Bidirectional(layers.LSTM(128, return_sequences = True))(seq_input)\n",
    "#     tensor_var = layers.Dropout(0.15)(tensor_var)\n",
    "#     tensor_var = layers.BatchNormalization()(tensor_var)\n",
    "    tensor_var = layers.Bidirectional(layers.LSTM(128, return_sequences = True))(tensor_var)\n",
    "    tensor_var = layers.BatchNormalization()(tensor_var)\n",
    "#     tensor_var = layers.TimeDistributed(layers.Dense(10))(tensor_var)\n",
    "#     tensor_var = layers.TimeDistributed(layers.LeakyReLU())(tensor_var)\n",
    "    \n",
    "    \n",
    "    #  if not 1-to-1, fully-connected layer across time to generate outputs\n",
    "    if ty < tx:\n",
    "        tensor_var = layers.Dense(ty, activation='relu')(tensor_var)\n",
    "        Y = layers.Dense(n_y)(tensor_var)\n",
    "    else:\n",
    "        Y = layers.TimeDistributed(layers.Dense(n_y))(tensor_var)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "sOFtw2tuIxLJ",
    "outputId": "9a8f9a85-d095-4782-e5ac-b9e654083ea1"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, sequence_length, batch_size=256, sequence_stride=1,\n",
    "                 shuffle=None, fit=True, output_sequence=True, output_cols=None,\n",
    "                 mini_batch_limit=np.inf):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.shuffle = fit if shuffle is None else shuffle\n",
    "        self.fit = fit\n",
    "        self.output_sequence = output_sequence\n",
    "        self.pad_value = 0.\n",
    "        self.mini_batch_limit = mini_batch_limit\n",
    "        self.indexes = []\n",
    "        if output_cols is None:\n",
    "            self.output_cols = data[0][1].columns\n",
    "        else:\n",
    "            self.output_cols = output_cols\n",
    "        for si, s in enumerate(data):\n",
    "            x = s[0]\n",
    "            tx = x.shape[0]\n",
    "            xind = 0\n",
    "            while tx > sequence_length:\n",
    "                self.indexes.append((si, xind))\n",
    "                xind += sequence_stride\n",
    "                tx -= sequence_stride\n",
    "            self.indexes.append((si,xind))\n",
    "        np.random.shuffle(self.indexes)  # always shuffle once\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.min([len(self.indexes) / self.batch_size, self.mini_batch_limit]))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.batch_size\n",
    "        this_size = self.batch_size if index + self.batch_size < len(self.indexes) else len(self.indexes) - index\n",
    "        X = np.zeros((this_size, self.sequence_length, self.data[0][0].shape[1]))\n",
    "        Y = np.zeros((this_size, self.sequence_length, len(self.output_cols)))\n",
    "        for i in range(this_size):\n",
    "            X[i,:,:], Y[i,:,:] = self.__getsingleitem(index + i)\n",
    "        if self.fit:\n",
    "            if self.output_sequence:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y[:,-1,:]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def __getsingleitem(self, index):\n",
    "        (seq, stride) = self.indexes[index]\n",
    "        (X, Y, _, _, _) = self.data[seq]\n",
    "        Y = Y.loc[:, self.output_cols]\n",
    "        if stride+self.sequence_length <= X.shape[0]:\n",
    "            if self.fit:\n",
    "                X = X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "                if self.output_sequence:\n",
    "                    Y = Y.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "                else:\n",
    "                    Y = Y.iloc[stride+self.sequence_length-1, :].to_numpy(dtype='float64').reshape((1,len(self.output_cols)))\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "        else:\n",
    "            # pad\n",
    "            X = X.iloc[stride:X.shape[0], :].to_numpy(dtype='float64')\n",
    "            padX = np.full((self.sequence_length - X.shape[0], X.shape[1]), self.pad_value)\n",
    "            if self.fit:\n",
    "                if self.output_sequence:\n",
    "                    Y = Y.iloc[stride:Y.shape[0], :].to_numpy(dtype='float64')\n",
    "                    padY = np.full((self.sequence_length - Y.shape[0], Y.shape[1]), self.pad_value)\n",
    "                    return np.concatenate((X, padX), axis=0), np.concatenate((Y, padY), axis=0)\n",
    "                else:\n",
    "                    padY = np.full((1, Y.shape[1]), self.pad_value)\n",
    "            else:\n",
    "                return np.concatenate((X, padX), axis=0).reshape((1, self.sequence_length, X.shape[1]))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "np.random.seed(1728)\n",
    "seq_length = 200\n",
    "output_cols = ['ioiRatio']\n",
    "\n",
    "np.random.shuffle(sequences)  # shuffle before splitting validation set\n",
    "val_split_ix = int(0.9*len(sequences))\n",
    "train = []\n",
    "val = []\n",
    "for (s, _) in sequences[:val_split_ix]:\n",
    "    train += s\n",
    "for (sv, p) in sequences[val_split_ix:]:\n",
    "    for (x, y, tr, i, mm) in sv:\n",
    "        if tr == 0: #  no transposition\n",
    "            val += [(x, y, p, i, mm)]\n",
    "\n",
    "# sequences = None  # if you need a bit more memory, to allow garbage collection\n",
    "\n",
    "# uncomment to reduce ds for testing\n",
    "# train = train[:10]\n",
    "# val = val[:10]\n",
    "\n",
    "generator = DataGenerator(train, seq_length, output_sequence=True, output_cols=output_cols)\n",
    "                          #shuffle=False, mini_batch_limit=50)\n",
    "val_gen = DataGenerator(val, seq_length, output_sequence=True, output_cols=output_cols)\n",
    "                        #shuffle=False, mini_batch_limit=25)\n",
    "\n",
    "model = my_model(seq_length, seq_length, train[0][0].shape[1], len(output_cols),\n",
    "                 train[0][0].columns.get_loc(\"melody\"), len(ix_to_lex) + 2)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-3)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB1zXiP9IxLV"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "JpghmUE3IxLX",
    "outputId": "6c2e7186-6bf7-41ac-ea28-387da0e9d393"
   },
   "outputs": [],
   "source": [
    "model.fit(generator, epochs=1, validation_data=val_gen)\n",
    "\n",
    "#  Save model\n",
    "# model.save_weights(pathRoot + '2020-08-31_timing.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mv-wgSVTIxLj"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2eV2YV0IxL1",
    "outputId": "2af81d2a-bd9b-47a8-a0d3-038baaf6e948"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "# model.load_weights(pathRoot + '2020-08-31_timing.h5')\n",
    "\n",
    "#  Compute note-level error\n",
    "\n",
    "# validation data\n",
    "test_sequences = val\n",
    "\n",
    "# test data\n",
    "# with open(pathRoot + 'note_sequences_test.data', 'rb') as seq_path:\n",
    "#     test_sequences = pickle.load(seq_path)\n",
    "#     ts = []\n",
    "#     for (sv, p) in test_sequences:\n",
    "#         for (x, y, tr, i, mm) in sv:\n",
    "#             if tr == 0:\n",
    "#                 ts.append((x, y, p, i, mm))\n",
    "#     test_sequences = ts\n",
    "\n",
    "def evaluation(sequences, sequence_length, model, pad_value=0.):\n",
    "    Yhat = []\n",
    "    for S in sequences:\n",
    "        X = S[0]\n",
    "        tx = X.shape[0]\n",
    "        n_x = int(tx / sequence_length)\n",
    "        n_x += 0 if tx % sequence_length == 0 else 1\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        for i in range(n_x - 1):            \n",
    "            x[i,:,:] = X.iloc[(i * sequence_length):(i + 1) * sequence_length,:].to_numpy()\n",
    "        x[n_x - 1,:tx - (n_x - 1) * sequence_length,:] = X.iloc[(n_x - 1) * sequence_length:,:].to_numpy()\n",
    "        y = model.predict(x)\n",
    "        Yhat.append(y.reshape((-1,y.shape[2])))\n",
    "    return Yhat\n",
    "\n",
    "def sliding_evaluation(sequences, sequence_length, model, pad_value=0., pad_start=True):\n",
    "    Yhat = []\n",
    "    for S in sequences:\n",
    "        X = S[0]\n",
    "        tx = X.shape[0]\n",
    "        n_x = tx if pad_start else tx - sequence_length + 1\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        idx = 0\n",
    "        if pad_start:\n",
    "            for i in range(0, sequence_length):\n",
    "                x[i,sequence_length-i-1:,:] = X.iloc[0:i+1,:].to_numpy()\n",
    "            idx = sequence_length\n",
    "        else:\n",
    "            x[0,:,:] = X.iloc[0:sequence_length,:].to_numpy()\n",
    "            idx = 1\n",
    "        for i in range(1, tx - sequence_length):\n",
    "            x[idx,:,:] = X.iloc[i:i+sequence_length,:].to_numpy()\n",
    "            idx += 1\n",
    "        y = model.predict(x)\n",
    "        if y.ndim < 3:  # single timestep prediction\n",
    "            Yhat.append(y)\n",
    "        elif pad_start:\n",
    "            Yhat.append(y[:,-1,:])\n",
    "        else:\n",
    "            Yhat.append(np.concatenate((y[0,:,:], y[1:, -1, :])))\n",
    "    return Yhat\n",
    "\n",
    "Yhat = evaluation(test_sequences, seq_length, model)\n",
    "mse = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "ms = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "for i, (_, Y, _, _, _) in enumerate(test_sequences):\n",
    "    Y = Y.loc[:,output_cols]\n",
    "    mse[i,:] = np.mean((Yhat[i][:Y.shape[0],:] - Y) ** 2)\n",
    "    ms[i,:] = np.mean(Y ** 2)\n",
    "    \n",
    "print('Validation set MSE for y_0: ' + str(np.mean(mse[:,0])) + '     mean square val: ' + str(np.mean(ms[:,0])))\n",
    "print('Minimum y_0 MSE among pieces: ' + str(mse[:,0].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mse[:,0])\n",
    "plt.plot(ms[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "piece = 0\n",
    "attr = ['ioiRatio']\n",
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(Yhat[piece][:,0])\n",
    "plt.plot(test_sequences[piece][1].iloc[:,attr].to_numpy())\n",
    "# print(test_sequences[piece][1].columns[attr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listen to a synthesized predicted expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import IPython.display\n",
    "\n",
    "# piece to synthesize:\n",
    "pieceNum = 21\n",
    "pieceId = test_sequences[pieceNum][2]\n",
    "print(pieceId)\n",
    "\n",
    "# TODO : change into generation from ioiRatio\n",
    "\n",
    "# deviations_pred = Yhat[pieceNum][:,0]*moments['timingDevLocal'][1] + moments['timingDevLocal'][0]\n",
    "deviations_perf = test_sequences[pieceNum][1].timingDevLocal*moments['timingDevLocal'][1] + moments['timingDevLocal'][0]\n",
    "tempo = test_sequences[pieceNum][1].localTempo.iloc[0] * moments['localTempo'][1] + moments['localTempo'][0]\n",
    "pm = pretty_midi.PrettyMIDI(initial_tempo=60 * tempo)\n",
    "inst = pretty_midi.Instrument(program=test_sequences[pieceNum][3], is_drum=False, name='melody_inst')\n",
    "pm.instruments.append(inst)\n",
    "start = 0.\n",
    "lastNote = None\n",
    "for x, y, dev in zip(test_sequences[pieceNum][0].itertuples(), test_sequences[pieceNum][1].itertuples(), deviations_perf):\n",
    "    (pitch, _) = ix_to_lex[x.melody]\n",
    "    start = y.startTime * moments['startTime'][1] + moments['startTime'][0]\n",
    "    if lastNote:\n",
    "        if start < lastNote.end:\n",
    "            lastNote.end = start\n",
    "    end = start + (y.durationSecs * moments['durationSecs'][1] + moments['durationSecs'][0])\n",
    "    lastNote = pretty_midi.Note(100, pitch, start, end)\n",
    "    inst.notes.append(lastNote)\n",
    "#     start += (x.ioi * moments['ioi'][1] + moments['ioi'][0]) / tempo\n",
    "IPython.display.Audio(pm.fluidsynth(fs=44100), rate=44100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PhraseDynamicsLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
