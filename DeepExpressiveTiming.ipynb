{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs2Yk5y3IxKb"
   },
   "source": [
    "Deep artificial neural network for expressive timing predictions in musical pieces\n",
    "---------------\n",
    "\n",
    "This notebook loads the data generated from the note level processing notebook and uses them to train and test a long sequence-based artificial neural network for predicting the onset timing deviation of notes from the MusicNet dataset pieces.\n",
    "\n",
    "\n",
    "#### Load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5sQCWWMtIxKg",
    "outputId": "c0937385-1926-4992-ccd9-6dba03c3e71a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "np.random.seed(1728)\n",
    "\n",
    "#  read dataset\n",
    "runLocal = True\n",
    "if runLocal:\n",
    "    pathRoot = 'data/'\n",
    "else:\n",
    "    pathRoot = '/content/drive/My Drive/colab_data/'\n",
    "\n",
    "with open(pathRoot + 'note_sequences_voc.data', 'rb') as seq_path:\n",
    "    sequences = pickle.load(seq_path)\n",
    "with open(pathRoot + 'note_sequences_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "    ix_to_lex = {v: k for k, v in lex_to_ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzL3Y9MOIxLG"
   },
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "def my_model(tx, ty, n_x, n_y, vocab_col, vocab_size):\n",
    "    X = Input((tx, n_x))\n",
    "    \n",
    "    #  Split the input vector between one-hot and numerical features\n",
    "    mk = list(range(n_x))\n",
    "    mk.remove(vocab_col)\n",
    "    \n",
    "    emb_input = layers.Lambda(lambda x: x[:, :, vocab_col])(X)\n",
    "    num_input = layers.Lambda(lambda x: tf.gather(x, mk, axis=2))(X)\n",
    "\n",
    "    #  Compute an embedding vector and combine it with the numeric features\n",
    "    emb_vec = layers.Embedding(input_dim=vocab_size, output_dim=16, mask_zero=True)(emb_input)\n",
    "    seq_input = layers.Concatenate(axis=2)([emb_vec, num_input])\n",
    "    \n",
    "    #  Run a sequence model\n",
    "    tensor_var = layers.BatchNormalization()(seq_input)\n",
    "    tensor_var = layers.Bidirectional(layers.LSTM(64, return_sequences = True))(tensor_var)\n",
    "    tensor_var = layers.Dropout(0.1)(tensor_var)\n",
    "    tensor_var = layers.BatchNormalization()(tensor_var)\n",
    "    tensor_var = layers.TimeDistributed(layers.Dense(10))(tensor_var)\n",
    "    tensor_var = layers.TimeDistributed(layers.LeakyReLU())(tensor_var)\n",
    "    \n",
    "    \n",
    "    #  if not 1-to-1, fully-connected layer across time to generate outputs\n",
    "    if ty < tx:\n",
    "        tensor_var = layers.Dense(ty, activation='relu')(tensor_var)\n",
    "        Y = layers.Dense(n_y)(tensor_var)\n",
    "    else:\n",
    "        Y = layers.TimeDistributed(layers.Dense(n_y))(tensor_var)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "sOFtw2tuIxLJ",
    "outputId": "9a8f9a85-d095-4782-e5ac-b9e654083ea1"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, sequence_length, batch_size=25, sequence_stride=1,\n",
    "                 shuffle=None, fit=True, output_sequence=True, output_cols=None):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.shuffle = fit if shuffle is None else shuffle\n",
    "        self.fit = fit\n",
    "        self.output_sequence = output_sequence\n",
    "        self.pad_value = 0.\n",
    "        self.indexes = []\n",
    "        if output_cols is None:\n",
    "            self.output_cols = data[0][1].columns\n",
    "        else:\n",
    "            self.output_cols = output_cols\n",
    "        for si, (x, _,) in enumerate(data):\n",
    "            tx = x.shape[0]\n",
    "            xind = 0\n",
    "            while tx > sequence_length:\n",
    "                self.indexes.append((si, xind))\n",
    "                xind += sequence_stride\n",
    "                tx -= sequence_stride\n",
    "            self.indexes.append((si,xind))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.indexes) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.batch_size\n",
    "        this_size = self.batch_size if index + self.batch_size < len(self.indexes) else len(self.indexes) - index\n",
    "        X = np.zeros((this_size, self.sequence_length, self.data[0][0].shape[1]))\n",
    "        Y = np.zeros((this_size, self.sequence_length, len(self.output_cols)))\n",
    "        for i in range(this_size):\n",
    "            X[i,:,:], Y[i,:,:] = self.__getsingleitem(index + i)\n",
    "        if self.fit:\n",
    "            if self.output_sequence:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y[:,-1,:]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def __getsingleitem(self, index):\n",
    "        (seq, stride) = self.indexes[index]\n",
    "        (X, Y) = self.data[seq]\n",
    "        Y = Y.loc[:, self.output_cols]\n",
    "        if stride+self.sequence_length <= X.shape[0]:\n",
    "            if self.fit:\n",
    "                X = X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "                if self.output_sequence:\n",
    "                    Y = Y.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "                else:\n",
    "                    Y = Y.iloc[stride+self.sequence_length-1, :].to_numpy(dtype='float64').reshape((1,len(self.output_cols)))\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "        else:\n",
    "            # pad\n",
    "            X = X.iloc[stride:X.shape[0], :].to_numpy(dtype='float64')\n",
    "            padX = np.full((self.sequence_length - X.shape[0], X.shape[1]), self.pad_value)\n",
    "            if self.fit:\n",
    "                if self.output_sequence:\n",
    "                    Y = Y.iloc[stride:Y.shape[0], :].to_numpy(dtype='float64')\n",
    "                    padY = np.full((self.sequence_length - Y.shape[0], Y.shape[1]), self.pad_value)\n",
    "                    return np.concatenate((X, padX), axis=0), np.concatenate((Y, padY), axis=0)\n",
    "                else:\n",
    "                    padY = np.full((1, Y.shape[1]), self.pad_value)\n",
    "            else:\n",
    "                return np.concatenate((X, padX), axis=0).reshape((1, self.sequence_length, X.shape[1]))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "seq_length = 60\n",
    "output_cols = ['timingDevLocal', 'peakLevel']\n",
    "np.random.shuffle(sequences)  # shuffle before splitting validation set\n",
    "# small ds for testing\n",
    "# sequences = sequences[:10]\n",
    "val_split_ix = int(0.9*len(sequences))\n",
    "generator = DataGenerator(sequences[:val_split_ix], seq_length, output_sequence=True, output_cols=output_cols)\n",
    "val_gen = DataGenerator(sequences[val_split_ix:], seq_length, output_sequence=True, output_cols=output_cols)\n",
    "\n",
    "model = my_model(seq_length, seq_length, sequences[0][0].shape[1], len(output_cols),\n",
    "                 sequences[0][0].columns.get_loc(\"melody\"), len(ix_to_lex) + 2)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB1zXiP9IxLV"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "JpghmUE3IxLX",
    "outputId": "6c2e7186-6bf7-41ac-ea28-387da0e9d393"
   },
   "outputs": [],
   "source": [
    "model.fit(generator, epochs=3, validation_data=val_gen)\n",
    "\n",
    "#  Save model\n",
    "# model.save_weights('timing2020-08-03_timing.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mv-wgSVTIxLj"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2eV2YV0IxL1",
    "outputId": "2af81d2a-bd9b-47a8-a0d3-038baaf6e948"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "#model.load_weights(pathRoot + 'timing2020-08-10.h5')\n",
    "\n",
    "#  Compute note-level error\n",
    "\n",
    "# validation data\n",
    "test_sequences = sequences[val_split_ix:]\n",
    "\n",
    "# test data\n",
    "# with open(pathRoot + 'note_sequences_test_voc.data', 'rb') as seq_path:\n",
    "#     test_sequences = pickle.load(seq_path)\n",
    "\n",
    "def evaluation(sequences, sequence_length, model, pad_value=0.):\n",
    "    Yhat = []\n",
    "    for (X, _) in sequences:\n",
    "        tx = X.shape[0]\n",
    "        n_x = int(tx / sequence_length)\n",
    "        n_x += 0 if tx % sequence_length == 0 else 1\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        for i in range(n_x - 1):            \n",
    "            x[i,:,:] = X.iloc[(i * sequence_length):(i + 1) * sequence_length,:].to_numpy()\n",
    "        x[n_x - 1,:tx - (n_x - 1) * sequence_length,:] = X.iloc[(n_x - 1) * sequence_length:,:].to_numpy()\n",
    "        y = model.predict(x)\n",
    "        Yhat.append(y.reshape((-1,y.shape[2])))\n",
    "    return Yhat\n",
    "\n",
    "def sliding_evaluation(sequences, sequence_length, model, pad_value=0., pad_start=True):\n",
    "    Yhat = []\n",
    "    for (X, _) in sequences:\n",
    "      tx = X.shape[0]\n",
    "      n_x = tx if pad_start else tx - sequence_length + 1\n",
    "      x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "      if pad_start:\n",
    "        r = range(0, tx - seq_length)\n",
    "      else:\n",
    "        x[0,:,:] = X.iloc[0:seq_length,:].to_numpy()\n",
    "        r = range(seq_length, tx - seq_length)\n",
    "      for i in r:\n",
    "        x[i,:,:] = X.iloc[i:i+seq_length,:].to_numpy()\n",
    "      y = model.predict(x)\n",
    "      if y.ndim < 3:  # single timestep prediction\n",
    "        Yhat.append(y)\n",
    "      elif pad_start:\n",
    "        Yhat.append(y[:,-1,:])\n",
    "      else:\n",
    "        Yhat.append(np.concatenate((y[0,:,:], y[1:, -1, :])))\n",
    "    return Yhat\n",
    "\n",
    "Yhat = sliding_evaluation(test_sequences, seq_length, model)\n",
    "mse = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "ms = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "for i, (_, Y) in enumerate(test_sequences):\n",
    "    Y = Y.loc[:,output_cols]\n",
    "    mse[i,:] = np.mean((Yhat[i][:Y.shape[0],:] - Y) ** 2)\n",
    "    ms[i,:] = np.mean(Y ** 2)\n",
    "    \n",
    "print('Validation set MSE for y_0: ' + str(np.mean(mse[:,0])) + '     mean square val: ' + str(np.mean(ms[:,0])))\n",
    "print('Minimum y_0 MSE among pieces: ' + str(mse[:,0].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mse[:,1])\n",
    "plt.plot(ms[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "piece = 5\n",
    "attr = 1 # from 0 to 2\n",
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(Yhat[piece][:,0])\n",
    "plt.plot(test_sequences[piece][1].iloc[:,attr].to_numpy())\n",
    "print(test_sequences[piece][1].columns[attr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PhraseDynamicsLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
