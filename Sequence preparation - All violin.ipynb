{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note-level dataset generation\n",
    "\n",
    "This notebook uses raw data from the MusicNet dataset to set up sequential numpy arrays suitable for training deep neural networks.\n",
    "\n",
    "**Before running:** Make sure to run the \"Levels Computation\" notebook to produce the numpy array files with global audio levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START HERE ####\n",
    "\n",
    "dataFolder = 'data/'  # make sure the path to data folder is correct\n",
    "num_folds = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing piece 123/123\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import expression_modeling as m \n",
    "\n",
    "def preprocess(labelsDir, csv, outfile=None, include_header=False, include_transp=True):\n",
    "        \n",
    "    # load the symbolic information from the dataset\n",
    "    notearray = np.genfromtxt(os.path.join(labelsDir, csv + '.csv'), delimiter=',', names=True, dtype=['i', 'i', 'i', 'i', 'f', 'f', '|U40'])\n",
    "\n",
    "    # sort by score time first for correct parsing\n",
    "    notearray.sort(order=['start_beat', 'start_time'])\n",
    "\n",
    "    # load levels (generated by \"Levels computation\" notebook)\n",
    "    levels = np.load(os.path.join(dataFolder, 'levels', csv + '_global_lvls.npy'))\n",
    "\n",
    "    piece = m.Piece(name=csv)\n",
    "    piece.dynMean = np.mean(levels)\n",
    "    piece.dynStd = np.std(levels)\n",
    "    piece.startTime = notearray['start_time'][0]\n",
    "    piece.startBeat = notearray['start_beat'][0]\n",
    "    piece.endTime = notearray['end_time'][-1]\n",
    "    piece.endBeat = notearray['start_beat'][-1] + notearray['end_beat'][-1]\n",
    "    piece.part = m.buildPart(notearray, (levels - piece.dynMean)/piece.dynStd, 44100)\n",
    "\n",
    "    df = []\n",
    "    if include_transp:\n",
    "        for tr in range(-3,4):\n",
    "            di = m.buildSimpleNoteDataframe(piece, transpose=tr)\n",
    "            di['transposition'] = tr\n",
    "            df.append(di)\n",
    "    else:\n",
    "        df = m.buildSimpleNoteDataframe(piece, transpose=0)\n",
    "        df['transposition'] = 0\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    df['pieceId'] = int(csv)\n",
    "\n",
    "    if outfile is None:\n",
    "        outfile = open(os.path.join(dataFolder, csv + '.csv'), 'w+')\n",
    "        df.to_csv(outfile)\n",
    "    else:\n",
    "        df.to_csv(outfile, mode='a', header=include_header)\n",
    "    return outfile\n",
    "\n",
    "# select pieces containing violin\n",
    "csvfolder = os.path.join(dataFolder, 'musicnet', 'train_labels')\n",
    "dataset = [csv for csv in os.listdir(csvfolder) if re.search(r'^(.*?,){2}\\s*?41\\s*?,(.*?,){3}', open(os.path.join(csvfolder, csv), 'r').read(), re.MULTILINE)]\n",
    "\n",
    "with open(os.path.join(dataFolder, 'musicnet_violin_IE1.csv'), 'w+') as outfile:\n",
    "    header = True\n",
    "    for i,mvt in enumerate(dataset):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Processing piece \" + str(i+1) + '/' + str(len(dataset)))\n",
    "        preprocess(csvfolder, mvt[:-4], outfile, include_header=header)\n",
    "        header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation (Features)\n",
    "\n",
    "The following cells use the CSV file produced above to format the data into sequences of notes containing a pitch vocabulary and a set of musicologically relevant features about the note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: 3672648\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "with open(os.path.join(dataFolder, 'musicnet_violin_IE1.csv'), 'r') as path:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "# Input Encoding I --> minimal, i.e.: no musicological info\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "instrs = set(df.instrument)\n",
    "df['instrument'] = df['instrument'].astype(pd.CategoricalDtype(instrs))\n",
    "\n",
    "print('initial size: ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "\n",
    "# using mapping from maestro dataset (88 keys + 4 ctrl. words)\n",
    "with open(os.path.join(dataFolder, 'mF_pitch_dict.data'), 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "df['pitch'] = [lex_to_ix.get(m, len(lex_to_ix)+1) for m in pitches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Training / Validation / Test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set pieces: ['2140', '1923', '2186', '2397', '1872', '2627', '1828', '2433', '2148', '2288', '1788', '2178']\n",
      "Val. set IDs: [['2398', '2562', '2573', '1739', '2166', '1919', '2497', '2482', '1805', '2243', '1729']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(777)\n",
    "\n",
    "csvfolder = os.path.join(dataFolder, 'musicnet', 'train_labels')\n",
    "all = [csv[0:-4] for csv in os.listdir(csvfolder) if re.search(r'^(.*?,){2}\\s*?41\\s*?,(.*?,){3}', open(os.path.join(csvfolder, csv), 'r').read(), re.MULTILINE)]\n",
    "\n",
    "folds = []\n",
    "test_pieces = []\n",
    "\n",
    "# reserve 10% for test\n",
    "test_sz = int(len(all) / 10)\n",
    "for i in range(test_sz):\n",
    "    m = all[random.randint(0, len(all) - 1)]\n",
    "    test_pieces.append(m)\n",
    "    all.remove(m)\n",
    "print('Test set pieces: ' + str(test_pieces))\n",
    "      \n",
    "for i in range(num_folds):\n",
    "    train = all.copy()\n",
    "    val = []\n",
    "    # another 10% of what remains for validation\n",
    "    val_sz = int(len(train) / 10)\n",
    "    for j in range(val_sz):\n",
    "        m = train[random.randint(0, len(train) - 1)]\n",
    "        val.append(m)\n",
    "        train.remove(m)\n",
    "    folds.append((train, val))\n",
    "\n",
    "print('Val. set IDs: ' + str([f[1] for f in folds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging data for sequential training and saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fold 0\n",
      "Saving test data\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def sequencer(df, one_hot_cols=None, include_transp=True):\n",
    "    sequences = []\n",
    "    maxLen = 0\n",
    "    # list the pieces\n",
    "    pieces = set(df.pieceId)\n",
    "    for p in pieces:\n",
    "        dp = df.loc[df.pieceId == p, :].copy()\n",
    "        transps = range(-3,4) if include_transp else [0]\n",
    "        for tr in transps:\n",
    "            d = dp.loc[dp.transposition == tr, :].copy()\n",
    "            maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "            d.drop(['pieceId', 'transposition'], axis=1, inplace=True)\n",
    "\n",
    "            # convert categories to one-hot\n",
    "            if one_hot_cols:\n",
    "                for attrib in one_hot_cols:\n",
    "                    d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "                    d.drop([attrib], axis=1, inplace=True)\n",
    "\n",
    "            # instance standardization for relevant features\n",
    "            feats = ['localTempo', 'peakLevel', 'ioiRatio']\n",
    "            aux = d.loc[:, feats]\n",
    "            moments = np.zeros((aux.shape[1], 2))\n",
    "            moments[:, 0] = aux.mean().to_numpy()\n",
    "            moments[:, 1] = aux.std().to_numpy()\n",
    "            d.loc[:, feats] = (aux - moments[:,0])/ moments[:,1]\n",
    "\n",
    "            # add <END> token to sequence\n",
    "            end = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "            end[\"pitch\"] = len(lex_to_ix) + 2\n",
    "            d = d.append(end)\n",
    "\n",
    "            # add <SOS> token to sequence\n",
    "            start = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "            start[\"pitch\"] = len(lex_to_ix) + 3\n",
    "            d = pd.concat([start, d])\n",
    "            \n",
    "            # separate output features\n",
    "            outCols = ['ioiRatio', 'timingDev', 'timingDevLocal', 'localTempo', 'peakLevel', 'startTime', 'durationSecs']\n",
    "            y = d.loc[:, outCols].copy()\n",
    "            d.drop(outCols, axis=1, inplace=True)\n",
    "\n",
    "            sequences.append(((d, y, moments), p, tr))\n",
    "    return sequences\n",
    "\n",
    "def standardize(df, moments=None, cols=None):\n",
    "    if cols is None:\n",
    "        cols = (df.dtypes == 'float64')\n",
    "    nums = df.loc[:,cols]\n",
    "    if moments is None:\n",
    "        moments = np.zeros((nums.shape[1],2))  # output mean and std for reverting predictions\n",
    "        moments[:,0] = nums.mean().to_numpy()\n",
    "        moments[:,1] = nums.std().to_numpy()\n",
    "    df.loc[:, cols] = (nums - moments[:,0]) / moments[:,1]\n",
    "    return moments, cols\n",
    "\n",
    "\n",
    "# Separate Training / Validation / Test:\n",
    "\n",
    "test = df.loc[df.pieceId.isin(test_pieces), :].copy()\n",
    "\n",
    "moments = None\n",
    "cols = None\n",
    "for i, (training_pieces, val_pieces) in enumerate(folds):\n",
    "    \n",
    "    train = df.loc[df.pieceId.isin(training_pieces), :].copy()\n",
    "    val = df.loc[df.pieceId.isin(val_pieces), :].copy()\n",
    "\n",
    "    # Standardization\n",
    "    moments, cols = standardize(train, cols=['beatDiff', 'duration', 'ioi', 'startTime', 'durationSecs', 'timingDev', 'timingDevLocal'])\n",
    "    standardize(val, moments=moments, cols=cols)\n",
    "    with open(os.path.join(dataFolder, 'MNv_I_normalizer_fold_' + str(i) + '.data'), 'wb') as filehandle:\n",
    "        pickle.dump((moments, cols), filehandle)\n",
    "    \n",
    "    train_seq = sequencer(train, one_hot_cols=['instrument'])\n",
    "    val_seq = sequencer(val, one_hot_cols=['instrument'], include_transp=False)\n",
    "    \n",
    "    #  Save arrays\n",
    "    print('Saving fold ' + str(i))\n",
    "    with open(os.path.join(dataFolder, 'MNv_I_train_sequences_fold_' + str(i) + '.data'), 'wb') as filehandle:\n",
    "        pickle.dump(train_seq, filehandle)\n",
    "    with open(os.path.join(dataFolder, 'MNv_I_val_sequences_fold_' + str(i) + '.data'), 'wb') as filehandle:\n",
    "        pickle.dump(val_seq, filehandle)\n",
    "\n",
    "# Prepare test sequences\n",
    "print('Saving test data')\n",
    "standardize(test, moments=moments, cols=cols) # using last fold moments (it's good enough)\n",
    "test_seq = sequencer(test, one_hot_cols=['instrument'], include_transp=False)\n",
    "with open(os.path.join(dataFolder, 'MNv_I_test_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(test_seq, filehandle)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
