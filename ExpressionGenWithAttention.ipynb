{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs2Yk5y3IxKb"
   },
   "source": [
    "Deep artificial neural network for expressive timing and dynamics predictions in musical pieces\n",
    "---------------\n",
    "\n",
    "This notebook loads the data generated from the note level processing notebook and uses them to train and test a long sequence-based artificial neural network for predicting the onset timing deviation and peak loudness level of notes from the MusicNet dataset pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters to set:\n",
    "\n",
    "runLocal = True  # False for using Google Colab\n",
    "BATCH_SIZE = 128\n",
    "seq_length = 200\n",
    "decoder_units = 128\n",
    "output_cols = ['peakLevel']\n",
    "lr = 3e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5sQCWWMtIxKg",
    "outputId": "c0937385-1926-4992-ccd9-6dba03c3e71a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#  read dataset\n",
    "\n",
    "if runLocal:\n",
    "    pathRoot = 'data/'\n",
    "else:\n",
    "    pathRoot = '/content/drive/My Drive/colab_data/'\n",
    "\n",
    "with open(pathRoot + 'note_sequences.data', 'rb') as seq_path:\n",
    "    sequences = pickle.load(seq_path)\n",
    "with open(pathRoot + 'note_sequences_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "    ix_to_lex = {v: k for k, v in lex_to_ix.items()}\n",
    "with open(pathRoot + 'normalizer.data', 'rb') as filehandle:\n",
    "    moments, cols = pickle.load(filehandle)\n",
    "    moments = dict(zip(cols, list(moments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing training/validation split\n",
    "\n",
    "np.random.seed(1728)\n",
    "\n",
    "np.random.shuffle(sequences)  # shuffle before splitting validation set\n",
    "val_split_ix = int(0.9*len(sequences))\n",
    "train = []\n",
    "val = []\n",
    "for (s, _) in sequences[:val_split_ix]:\n",
    "    train += s\n",
    "for (sv, p) in sequences[val_split_ix:]:\n",
    "    for (x, y, tr, i, mm) in sv:\n",
    "        if tr == 0: #  no transposition\n",
    "            val += [(x, y, p, i, mm)]\n",
    "\n",
    "# sequences = None  # if you need a bit more memory, to allow garbage collection\n",
    "\n",
    "# uncomment to reduce ds for testing\n",
    "train = train[:10]\n",
    "val = val[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzL3Y9MOIxLG"
   },
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, tx):\n",
    "        super(Attention, self).__init__()\n",
    "        self.repeat = layers.RepeatVector(tx)\n",
    "        self.concat = layers.Concatenate(axis=-1)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(10, activation=\"tanh\"), layers.Dense(1, activation = \"relu\"),\n",
    "            layers.Softmax()]\n",
    "        )\n",
    "        self.project = layers.Dot(axes = 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.repeat(inputs[0])\n",
    "        act = self.concat([query, inputs[1]])\n",
    "        act = self.ffn(act)\n",
    "        ctx = self.project([act, inputs[1]])\n",
    "        return ctx\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, tsteps, decoder_units, n_y):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tsteps = tsteps\n",
    "        self.decoder_units = decoder_units\n",
    "        self.att = Attention(tsteps)\n",
    "        self.rnn = layers.LSTM(decoder_units, return_state=True)\n",
    "        self.ff = tf.keras.Sequential(\n",
    "            [layers.Dense(10*n_y, activation=\"tanh\"), layers.Dense(n_y)]\n",
    "        )\n",
    "        self.reshaper = layers.Reshape((1, n_y))\n",
    "        self.cat = layers.Concatenate(axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.keras.backend.shape(inputs)[0]\n",
    "        h = tf.zeros((batch_size, self.decoder_units))\n",
    "        c = tf.zeros((batch_size, self.decoder_units))\n",
    "        y = []\n",
    "        for t in range(self.tsteps):\n",
    "            ctx = self.att([h, inputs])\n",
    "            h, _, c = self.rnn(ctx, initial_state=[h, c])\n",
    "            y.append(self.reshaper(self.ff(h)))\n",
    "        return self.cat(y)\n",
    "    \n",
    "def my_model(tx, ty, n_x, n_y, vocab_col, vocab_size, decoder_units):\n",
    "    X = Input((tx, n_x))\n",
    "    \n",
    "    #  Split the input vector between one-hot and numerical features\n",
    "    mk = list(range(n_x))\n",
    "    mk.remove(vocab_col)\n",
    "    \n",
    "    emb_input = layers.Lambda(lambda x: x[:, :, vocab_col])(X)\n",
    "    num_input = layers.Lambda(lambda x: tf.gather(x, mk, axis=2))(X)\n",
    "\n",
    "    #  Compute an embedding vector and combine it with the numeric features\n",
    "    emb_vec = layers.Embedding(input_dim=vocab_size, output_dim=64, mask_zero=True)(emb_input)\n",
    "    seq_input = layers.Concatenate(axis=2)([emb_vec, num_input])\n",
    "    \n",
    "    #  Encode\n",
    "    tensor_var = layers.Bidirectional(layers.LSTM(128, return_sequences = True))(seq_input)\n",
    "    tensor_var = layers.Dropout(0.15)(tensor_var)\n",
    "    tensor_var = layers.BatchNormalization()(tensor_var)\n",
    "    \n",
    "    #  Decode with Bahdanau-style attention\n",
    "    decoder = Decoder(ty, decoder_units, n_y)\n",
    "    Y = decoder(tensor_var)\n",
    "    \n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "sOFtw2tuIxLJ",
    "outputId": "9a8f9a85-d095-4782-e5ac-b9e654083ea1"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, sequence_length, batch_size=BATCH_SIZE, sequence_stride=1,\n",
    "                 shuffle=True, output_sequence=True, output_cols=None,\n",
    "                 mini_batch_limit=np.inf):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.shuffle = shuffle\n",
    "        self.output_sequence = output_sequence\n",
    "        self.pad_value = 0.\n",
    "        self.mini_batch_limit = mini_batch_limit\n",
    "        self.indexes = []\n",
    "        if output_cols is None:\n",
    "            self.output_cols = data[0][1].columns\n",
    "        else:\n",
    "            self.output_cols = output_cols\n",
    "        for si, s in enumerate(data):\n",
    "            x = s[0]\n",
    "            tx = x.shape[0]\n",
    "            xind = 0\n",
    "            while tx > sequence_length:\n",
    "                self.indexes.append((si, xind))\n",
    "                xind += sequence_stride\n",
    "                tx -= sequence_stride\n",
    "            self.indexes.append((si,xind))\n",
    "        np.random.shuffle(self.indexes)  # always shuffle once\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.min([len(self.indexes) / self.batch_size, self.mini_batch_limit]))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.batch_size\n",
    "        this_size = self.batch_size if index + self.batch_size < len(self.indexes) else len(self.indexes) - index\n",
    "        X = np.zeros((this_size, self.sequence_length, self.data[0][0].shape[1]))\n",
    "        Y = np.zeros((this_size, self.sequence_length, len(self.output_cols)))\n",
    "        for i in range(this_size):\n",
    "            X[i,:,:], Y[i,:,:] = self.__getsingleitem(index + i)\n",
    "        if self.output_sequence:\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X, Y[:,-1,:]\n",
    "    \n",
    "    def __getsingleitem(self, index):\n",
    "        (seq, stride) = self.indexes[index]\n",
    "        (X, Y, _, _, _) = self.data[seq]\n",
    "        Y = Y.loc[:, self.output_cols]\n",
    "        if stride+self.sequence_length <= X.shape[0]:\n",
    "            X = X.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "            if self.output_sequence:\n",
    "                Y = Y.iloc[stride:stride+self.sequence_length, :].to_numpy(dtype='float64')\n",
    "            else:\n",
    "                Y = Y.iloc[stride+self.sequence_length-1, :].to_numpy(dtype='float64').reshape((1,len(self.output_cols)))\n",
    "            return X, Y\n",
    "        else:\n",
    "            # pad\n",
    "            X = X.iloc[stride:X.shape[0], :].to_numpy(dtype='float64')\n",
    "            padX = np.full((self.sequence_length - X.shape[0], X.shape[1]), self.pad_value)\n",
    "            if self.output_sequence:\n",
    "                Y = Y.iloc[stride:Y.shape[0], :].to_numpy(dtype='float64')\n",
    "                padY = np.full((self.sequence_length - Y.shape[0], Y.shape[1]), self.pad_value)\n",
    "                return np.concatenate((X, padX), axis=0), np.concatenate((Y, padY), axis=0)\n",
    "            else:\n",
    "                padY = np.full((1, Y.shape[1]), self.pad_value)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator(train, seq_length, batch_size=BATCH_SIZE, output_sequence=True, output_cols=output_cols,\n",
    "                          shuffle=False, mini_batch_limit=20)\n",
    "val_gen = DataGenerator(val, seq_length, batch_size=BATCH_SIZE, output_sequence=True, output_cols=output_cols,\n",
    "                        shuffle=False, mini_batch_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(seq_length, seq_length, train[0][0].shape[1], len(output_cols),\n",
    "                 train[0][0].columns.get_loc(\"melody\"), len(ix_to_lex) + 3, decoder_units)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB1zXiP9IxLV"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "JpghmUE3IxLX",
    "outputId": "6c2e7186-6bf7-41ac-ea28-387da0e9d393"
   },
   "outputs": [],
   "source": [
    "model.fit(generator, epochs=1, validation_data=val_gen)\n",
    "\n",
    "#  Save model\n",
    "# model.save_weights(pathRoot + '2020-08-31_timing.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mv-wgSVTIxLj"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2eV2YV0IxL1",
    "outputId": "2af81d2a-bd9b-47a8-a0d3-038baaf6e948"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "# model.load_weights(pathRoot + '2020-09-03_huge.h5')\n",
    "\n",
    "#  Compute note-level error\n",
    "\n",
    "# validation data\n",
    "test_sequences = val\n",
    "\n",
    "# test data\n",
    "# with open(pathRoot + 'note_sequences_test.data', 'rb') as seq_path:\n",
    "#     test_sequences = pickle.load(seq_path)\n",
    "#     ts = []\n",
    "#     for (sv, p) in test_sequences:\n",
    "#         for (x, y, tr, i, mm) in sv:\n",
    "#             if tr == 0:\n",
    "#                 ts.append((x, y, p, i, mm))\n",
    "#     test_sequences = ts\n",
    "\n",
    "def evaluation(sequences, sequence_length, model, pad_value=0.):\n",
    "    Yhat = []\n",
    "    for S in sequences:\n",
    "        X = S[0]\n",
    "        tx = X.shape[0]\n",
    "        n_x = int(tx / sequence_length)\n",
    "        n_x += 0 if tx % sequence_length == 0 else 1\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        for i in range(n_x - 1):            \n",
    "            x[i,:,:] = X.iloc[(i * sequence_length):(i + 1) * sequence_length,:].to_numpy()\n",
    "        x[n_x - 1,:tx - (n_x - 1) * sequence_length,:] = X.iloc[(n_x - 1) * sequence_length:,:].to_numpy()\n",
    "        y = model.predict(x)\n",
    "        print(y.shape)\n",
    "        Yhat.append(y.reshape((-1,y.shape[2])))\n",
    "    return Yhat\n",
    "\n",
    "def sliding_evaluation(sequences, sequence_length, model, pad_value=0., pad_start=True):\n",
    "    Yhat = []\n",
    "    for S in sequences:\n",
    "        X = S[0]\n",
    "        tx = X.shape[0]\n",
    "        n_x = tx if pad_start else tx - sequence_length + 1\n",
    "        x = np.full((n_x, sequence_length, X.shape[1]), pad_value)\n",
    "        idx = 0\n",
    "        if pad_start:\n",
    "            for i in range(0, sequence_length):\n",
    "                x[i,sequence_length-i-1:,:] = X.iloc[0:i+1,:].to_numpy()\n",
    "            idx = sequence_length\n",
    "        else:\n",
    "            x[0,:,:] = X.iloc[0:sequence_length,:].to_numpy()\n",
    "            idx = 1\n",
    "        for i in range(1, tx - sequence_length):\n",
    "            x[idx,:,:] = X.iloc[i:i+sequence_length,:].to_numpy()\n",
    "            idx += 1\n",
    "        y = model.predict(x)\n",
    "        if y.ndim < 3:  # single timestep prediction\n",
    "            Yhat.append(y)\n",
    "        elif pad_start:\n",
    "            Yhat.append(y[:,-1,:])\n",
    "        else:\n",
    "            Yhat.append(np.concatenate((y[0,:,:], y[1:, -1, :])))\n",
    "    return Yhat\n",
    "\n",
    "Yhat = evaluation(test_sequences, seq_length, model)\n",
    "mse = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "ms = np.zeros((len(test_sequences), Yhat[0].shape[1]))\n",
    "for i, (_, Y, _, _, _) in enumerate(test_sequences):\n",
    "    Y = Y.loc[:,output_cols]\n",
    "    mse[i,:] = np.mean((Yhat[i][:Y.shape[0],:] - Y) ** 2)\n",
    "    ms[i,:] = np.mean(Y ** 2)\n",
    "    \n",
    "print('Validation set MSE for y_0: ' + str(np.mean(mse[:,0])) + '     mean square val: ' + str(np.mean(ms[:,0])))\n",
    "print('Minimum y_0 MSE among pieces: ' + str(mse[:,0].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mse[:,0])\n",
    "plt.plot(ms[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "piece = 0\n",
    "attr = ['peakLevel']\n",
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(Yhat[piece][:,0])\n",
    "plt.plot(test_sequences[piece][1].loc[:,attr].to_numpy())\n",
    "# print(test_sequences[piece][1].columns[attr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listen to a synthesized predicted expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import IPython.display\n",
    "\n",
    "test_sequences = val\n",
    "\n",
    "# piece to synthesize:\n",
    "pieceNum = 27\n",
    "pieceId = test_sequences[pieceNum][2]\n",
    "print(pieceId)\n",
    "\n",
    "deviations_pred = Yhat[pieceNum][:,0] * test_sequences[pieceNum][4][2,1] + test_sequences[pieceNum][4][2,0]\n",
    "deviations_perf = test_sequences[pieceNum][1].ioiRatio * test_sequences[pieceNum][4][2,1] + test_sequences[pieceNum][4][2,0]\n",
    "tempo = test_sequences[pieceNum][1].localTempo.iloc[0] * test_sequences[pieceNum][4][0,1] + test_sequences[pieceNum][4][0,0]\n",
    "no_dev = [test_sequences[pieceNum][4][2,0]] * test_sequences[pieceNum][1].shape[0]\n",
    "dev_rand = np.random.normal(size=test_sequences[pieceNum][1].shape[0]) * test_sequences[pieceNum][4][2,1] + test_sequences[pieceNum][4][2,0]\n",
    "pm = pretty_midi.PrettyMIDI(initial_tempo=60 * tempo)\n",
    "inst = pretty_midi.Instrument(program=test_sequences[pieceNum][3], is_drum=False, name='melody_inst')\n",
    "pm.instruments.append(inst)\n",
    "start = 0.\n",
    "lastNote = None\n",
    "for x, y, dev in zip(test_sequences[pieceNum][0].itertuples(), test_sequences[pieceNum][1].itertuples(), deviations_perf):\n",
    "    (pitch, _) = ix_to_lex[x.melody]\n",
    "    if lastNote:\n",
    "        if start < lastNote.end:\n",
    "            lastNote.end = start\n",
    "    end = start + (x.duration * moments['duration'][1] + moments['duration'][0]) * dev\n",
    "    lastNote = pretty_midi.Note(100, pitch, start, end)\n",
    "    inst.notes.append(lastNote)\n",
    "    start += (x.ioi * moments['ioi'][1] + moments['ioi'][0]) * dev\n",
    "IPython.display.Audio(pm.fluidsynth(fs=44100), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building conductive input from generated performance\n",
    "\n",
    "This step uses the predicted timing information to build a local tempo signal which can be used as input in a virtual conductor. That signal is compared to the local tempo vector obtained from the chosen reference performance from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PhraseDynamicsLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
