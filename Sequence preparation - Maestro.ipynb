{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note-level dataset generation\n",
    "\n",
    "This notebook uses raw data from the MAESTRO dataset to set up sequential numpy arrays suitable for training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START HERE ####\n",
    "\n",
    "dataFolder = 'data'  # make sure the path to data folder is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2017 piece 140/140\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import pretty_midi as pm\n",
    "\n",
    "import expression_modeling as m \n",
    "\n",
    "def preprocess(folder, file, outfile, piece_id, include_header=None):\n",
    "    \n",
    "    midi = pm.PrettyMIDI(os.path.join(folder, file + '.midi'))\n",
    "    df = m.buildNoteDataframeFromPerfMidi(midi)\n",
    "    df['pieceId'] = piece_id\n",
    "\n",
    "    if outfile is None:\n",
    "        if include_header is None:\n",
    "            include_header = True\n",
    "        outfile = open('maestro.csv', 'w+')\n",
    "        df.to_csv(outfile, header=include_header)\n",
    "    else:\n",
    "        if include_header is None:\n",
    "            include_header = False\n",
    "        df.to_csv(outfile, mode='a', header=include_header)\n",
    "    return outfile\n",
    "\n",
    "with open(os.path.join(dataFolder, 'maestroFull.csv'), 'w+') as outfile, open(os.path.join(dataFolder, 'maestroFull_ids.csv'), 'w+') as idfile:\n",
    "    header = True\n",
    "    for year in ['2004', '2006', '2008', '2009', '2011', '2013', '2014', '2015', '2017']:\n",
    "\n",
    "        files = [os.path.splitext(f)[0] for f in os.listdir(os.path.join(dataFolder, 'maestro', year)) if os.path.splitext(f)[1] == '.midi']\n",
    "        for i, f in enumerate(files):\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processing \" + year + \" piece \" + str(i+1) + '/' + str(len(files)))\n",
    "            idfile.write(\"{},{}\\n\".format(int(year)*1000 + i, year + '/' + f))\n",
    "            preprocess(os.path.join(dataFolder, 'maestro', year), f, outfile, int(year)*1000 + i, include_header=header)\n",
    "            header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation (Features)\n",
    "\n",
    "The following cells use the CSV file produced above to format the data into sequences of notes containing a pitch vocabulary and a set of features about the note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: 6176803\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "with open(os.path.join(dataFolder, 'maestroFull.csv'), 'r') as path:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "print('initial size: ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 88 + 4 ctl. words = 92\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "voc = list(set(pitches))\n",
    "print('vocabulary size = ' + str(len(voc)) + ' + 4 ctl. words = ' + str(4 + len(voc)))\n",
    "lex_to_ix = { lex:i+1 for i,lex in enumerate(voc) }  # index 0 is vacant for masking\n",
    "\n",
    "with open(os.path.join(dataFolder, 'mF_pitch_dict.data'), 'wb') as filehandle:\n",
    "    pickle.dump(lex_to_ix, filehandle)\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "df['pitch'] = [lex_to_ix.get(m, len(lex_to_ix)+1) for m in pitches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Training / Validation / Test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into:\n",
      "954 training pieces\n",
      "105 validation pieces\n",
      "125 test pieces\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(dataFolder, 'maestro', 'maestro-v1.0.0.csv')) as f:\n",
    "    mlst = pd.read_csv(f)\n",
    "with open(os.path.join(dataFolder, 'maestroFull_ids.csv')) as f:\n",
    "    mids = pd.read_csv(f, names=['id', 'name'])\n",
    "    \n",
    "names_tr = mlst.loc[mlst.split == 'train'].midi_filename\n",
    "names_tr = [n[:-5] for n in names_tr] # delete extension\n",
    "names_v = mlst.loc[mlst.split == 'validation'].midi_filename\n",
    "names_v = [n[:-5] for n in names_v]\n",
    "names_ts = mlst.loc[mlst.split == 'test'].midi_filename\n",
    "names_ts = [n[:-5] for n in names_ts]\n",
    "\n",
    "training_pieces = []\n",
    "validation_pieces = []\n",
    "test_pieces = []\n",
    "for m in mids.itertuples():\n",
    "    if m.name in names_tr:\n",
    "        training_pieces.append(m.id)\n",
    "    elif m.name in names_v:\n",
    "        validation_pieces.append(m.id)\n",
    "    elif m.name in names_ts:\n",
    "        test_pieces.append(m.id)\n",
    "    else:\n",
    "        print(m.name + ' not in any set.')\n",
    "        training_pieces.append(m.id)\n",
    "\n",
    "print('Split dataset into:')\n",
    "print(str(len(training_pieces)) + ' training pieces')\n",
    "print(str(len(validation_pieces)) + ' validation pieces')\n",
    "print(str(len(test_pieces)) + ' test pieces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging data for sequential training and saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def sequencer(df, one_hot_cols=None):\n",
    "    sequences = []\n",
    "    maxLen = 0\n",
    "    # list the pieces\n",
    "    pieces = set(df.pieceId)\n",
    "    for p in pieces:\n",
    "        d = df.loc[df.pieceId == p, :].copy()\n",
    "        maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "        d.drop(['pieceId'], axis=1, inplace=True)\n",
    "\n",
    "        # convert categories to one-hot\n",
    "        if one_hot_cols:\n",
    "            for attrib in one_hot_cols:\n",
    "                d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "                d.drop([attrib], axis=1, inplace=True)\n",
    "\n",
    "        # instance standardization for relevant features\n",
    "        feats = ['velocity']\n",
    "        aux = d.loc[:, feats]\n",
    "        moments = np.zeros((aux.shape[1], 2))\n",
    "        moments[:, 0] = aux.mean().to_numpy()\n",
    "        moments[:, 1] = aux.std().to_numpy()\n",
    "        d.loc[:, feats] = (aux - moments[:,0])/ moments[:,1]\n",
    "\n",
    "        # add <END> token to sequence\n",
    "        end = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "        end[\"pitch\"] = len(lex_to_ix) + 2\n",
    "        d = d.append(end)\n",
    "\n",
    "        # add <SOS> token to sequence\n",
    "        start = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "        start[\"pitch\"] = len(lex_to_ix) + 3\n",
    "        d = pd.concat([start, d])\n",
    "\n",
    "        # separate output features\n",
    "        outCols = ['velocity']\n",
    "        y = d.loc[:, outCols].copy()\n",
    "        d.drop(outCols, axis=1, inplace=True)\n",
    "\n",
    "        sequences.append(((d, y, moments), p, 0))\n",
    "    return sequences\n",
    "\n",
    "def standardize(df, moments=None, cols=None):\n",
    "    if cols is None:\n",
    "        cols = (df.dtypes == 'float64')\n",
    "    nums = df.loc[:,cols]\n",
    "    if moments is None:\n",
    "        moments = np.zeros((nums.shape[1],2))  # output mean and std for reverting predictions\n",
    "        moments[:,0] = nums.mean().to_numpy()\n",
    "        moments[:,1] = nums.std().to_numpy()\n",
    "    df.loc[:, cols] = (nums - moments[:,0]) / moments[:,1]\n",
    "    return moments, cols\n",
    "\n",
    "\n",
    "# Separate Training / Validation / Test:\n",
    "\n",
    "test = df.loc[df.pieceId.isin(test_pieces), :].copy()\n",
    "train = df.loc[df.pieceId.isin(training_pieces), :].copy()\n",
    "val = df.loc[df.pieceId.isin(validation_pieces), :].copy()\n",
    "\n",
    "# Standardization\n",
    "moments, cols = standardize(train, cols=['onsetDiff', 'durationSecs'])\n",
    "standardize(val, moments=moments, cols=cols)\n",
    "with open(os.path.join(dataFolder, 'mF_normalizer.data'), 'wb') as filehandle:\n",
    "    pickle.dump((moments, cols), filehandle)\n",
    "\n",
    "train_seq = sequencer(train)\n",
    "val_seq = sequencer(val)\n",
    "\n",
    "#  Save arrays\n",
    "print('Saving data')\n",
    "with open(os.path.join(dataFolder, 'mF_train_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(train_seq, filehandle)\n",
    "with open(os.path.join(dataFolder, 'mF_val_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(val_seq, filehandle)\n",
    "\n",
    "# Prepare test sequences\n",
    "standardize(test, moments=moments, cols=cols) # using last fold moments (it's good enough)\n",
    "test_seq = sequencer(test)\n",
    "with open(os.path.join(dataFolder, 'mF_test_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(test_seq, filehandle)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
