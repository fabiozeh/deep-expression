{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note-level dataset generation\n",
    "\n",
    "This notebook uses raw data from the MusicNet dataset to set up sequential numpy arrays suitable for training deep neural networks.\n",
    "\n",
    "**Before running:** Make sure to run the \"Levels Computation\" notebook to produce the numpy array files with global audio levels.\n",
    "\n",
    "If the intention is training a model in a remote server, instead of uploading the whole MusicNet dataset, the best strategy is to run the first two cells locally for generating the pandas csv, uploading that file and continuing from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START HERE ####\n",
    "\n",
    "dataFolder = 'data/'  # make sure the path to data folder is correct\n",
    "IE = 'II' # 'I', 'Ib', 'II', 'III'\n",
    "num_folds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing piece 21/21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import expression_modeling as m \n",
    "\n",
    "def preprocess(labelsDir, csv, outfile=None, include_header=False, include_transp=True):\n",
    "        \n",
    "    # load the symbolic information from the dataset\n",
    "    notearray = np.genfromtxt(os.path.join(labelsDir, csv + '.csv'), delimiter=',', names=True, dtype=['i', 'i', 'i', 'i', 'f', 'f', '|U40'])\n",
    "\n",
    "    # sort by score time first for correct parsing\n",
    "    notearray.sort(order=['start_beat', 'start_time'])\n",
    "\n",
    "    # load levels (generated by \"Levels computation\" notebook)\n",
    "    levels = np.load(os.path.join(dataFolder, 'levels', csv + '_global_lvls.npy'))\n",
    "\n",
    "    # piece key estimation (only major and minor for now)\n",
    "    isMajor, key, llhoodM, llhoodm = m.estimateKey(notearray['note'])\n",
    "    mode = m.Mode.major if isMajor else m.Mode.minor\n",
    "\n",
    "    # load time signature information\n",
    "    timesigdata = np.genfromtxt(os.path.join(dataFolder, 'musicnet', 'timesig.csv'), delimiter=',', names=True, dtype=['|U10', 'i', 'i', 'i'])\n",
    "    timesigdata = timesigdata[timesigdata['id'] == csv]\n",
    "    \n",
    "    piece = m.Piece(key=key, mode=mode, name=csv)\n",
    "    piece.beats_per_measure = timesigdata['upper'][0]\n",
    "    piece.time_sig_type = timesigdata['lower'][0]\n",
    "    piece.first_down_beat = timesigdata['pickup'][0]\n",
    "    piece.dynMean = np.mean(levels)\n",
    "    piece.dynStd = np.std(levels)\n",
    "    piece.startTime = notearray['start_time'][0]\n",
    "    piece.startBeat = notearray['start_beat'][0]\n",
    "    piece.endTime = notearray['end_time'][-1]\n",
    "    piece.endBeat = notearray['start_beat'][-1] + notearray['end_beat'][-1]\n",
    "    piece.part = m.buildPart(notearray, (levels - piece.dynMean)/piece.dynStd, 44100)\n",
    "\n",
    "    df = []\n",
    "    if include_transp:\n",
    "        for tr in range(-3,4):\n",
    "            di = m.buildNoteLevelDataframe(piece, transpose=tr)\n",
    "            di['transposition'] = tr\n",
    "            df.append(di)\n",
    "    else:\n",
    "        df = m.buildNoteLevelDataframe(piece, transpose=0)\n",
    "        df['transposition'] = 0\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    df['pieceId'] = int(csv)\n",
    "\n",
    "    if outfile is None:\n",
    "        outfile = open(os.path.join(dataFolder, csv + '.csv'), 'w+')\n",
    "        df.to_csv(outfile)\n",
    "    else:\n",
    "        df.to_csv(outfile, mode='a', header=include_header)\n",
    "    return outfile\n",
    "\n",
    "with open(os.path.join(dataFolder, 'musicnet', 'LvB_violinSonatas.json')) as f:\n",
    "    LvB = json.load(f)\n",
    "\n",
    "with open(os.path.join(dataFolder, 'LvB_violinSonatas.csv'), 'w+') as outfile:\n",
    "    mvt_list = []\n",
    "    header = True\n",
    "    for mvts in LvB.values():\n",
    "        mvt_list += mvts\n",
    "    for i,mvt in enumerate(mvt_list):\n",
    "        clear_output()\n",
    "        print(\"Processing piece \" + str(i+1) + '/' + str(len(mvt_list)))\n",
    "        preprocess(os.path.join(dataFolder, 'musicnet', 'train_labels'), str(mvt), outfile, include_header=header)\n",
    "        header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation (Features)\n",
    "\n",
    "The following cells use the CSV file produced above to format the data into sequences of notes containing a pitch vocabulary and a set of musicologically relevant features about the note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: 623623\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "with open(os.path.join(dataFolder, 'LvB_violinSonatas.csv'), 'r') as path:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "# Input Encoding I --> minimal, i.e.: no musicological info\n",
    "if IE == 'I':\n",
    "    df.drop(['Unnamed: 0', 'harmony', 'bassNote', \n",
    "             'probChord_I', 'probChord_II', 'probChord_III', 'probChord_IV',\n",
    "             'probChord_V', 'probChord_VI', 'probChord_VII', 'isDissonance',\n",
    "             'metricStrength'], axis=1, inplace=True)\n",
    "elif IE == 'Ib':\n",
    "    df.drop(['Unnamed: 0', 'harmony', 'bassNote', \n",
    "             'probChord_I', 'probChord_II', 'probChord_III', 'probChord_IV',\n",
    "             'probChord_V', 'probChord_VI', 'probChord_VII', 'isDissonance'],\n",
    "            axis=1, inplace=True)\n",
    "    df['metricStrength'] = df['metricStrength'].astype(pd.CategoricalDtype([0, 1, 2, 3]))\n",
    "elif IE == 'II':\n",
    "    df.drop(['Unnamed: 0', 'harmony', 'bassNote'], axis=1, inplace=True)\n",
    "    df['metricStrength'] = df['metricStrength'].astype(pd.CategoricalDtype([0, 1, 2, 3]))\n",
    "elif IE == 'III':\n",
    "    print('To be decided')\n",
    "else:\n",
    "    print('Invalid encoding requested')\n",
    "\n",
    "df['instrument'] = df['instrument'].astype(pd.CategoricalDtype([1, 41]))\n",
    "print('initial size: ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 81\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "voc = list(set(pitches))\n",
    "print('vocabulary size = ' + str(len(voc)))\n",
    "lex_to_ix = { lex:i+1 for i,lex in enumerate(voc) }  # index 0 is vacant for masking\n",
    "\n",
    "with open(os.path.join(dataFolder, 'LvB_pitch_dict.data'), 'wb') as filehandle:\n",
    "    pickle.dump(lex_to_ix, filehandle)\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "df['pitch'] = [lex_to_ix.get(m, len(lex_to_ix)+1) for m in pitches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Training / Validation / Test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set IDs: [2570, 2628]\n",
      "Val. set IDs: [[2629, 2398], [2572, 2334]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(777)\n",
    "\n",
    "with open(os.path.join(dataFolder, 'musicnet', 'LvB_violinSonatas.json')) as f:\n",
    "    LvB = json.load(f)\n",
    "    \n",
    "all = [mvt for sonata in LvB.values() for mvt in sonata]\n",
    "\n",
    "# remove repeated performances\n",
    "all.remove(2341)\n",
    "all.remove(2342)\n",
    "\n",
    "all.remove(2335)\n",
    "all.remove(2336) # these are more useful in training set\n",
    "folds = []\n",
    "test_pieces = []\n",
    "m = all[random.randint(0, len(all))]\n",
    "test_pieces.append(m)\n",
    "all.remove(m)\n",
    "m = all[random.randint(0, len(all))]\n",
    "test_pieces.append(m)\n",
    "all.remove(m)\n",
    "all.append(2335)\n",
    "all.append(2336)\n",
    "print('Test set IDs: ' + str(test_pieces))\n",
    "      \n",
    "for i in range(num_folds):\n",
    "    train = all.copy()\n",
    "    m = all[random.randint(0, len(train))]\n",
    "    val = [m]\n",
    "    train.remove(m)\n",
    "    m = all[random.randint(0, len(train))]\n",
    "    val.append(m)\n",
    "    train.remove(m)\n",
    "    if 2335 in val:\n",
    "        val.append(2341)\n",
    "    else:\n",
    "        train.append(2341)\n",
    "    if 2336 in val:\n",
    "        val.append(2342)\n",
    "    else:\n",
    "        train.append(2342)\n",
    "    folds.append((train, val))\n",
    "\n",
    "print('Val. set IDs: ' + str([f[1] for f in folds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging data for sequential training and saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fold 0\n",
      "Saving fold 1\n",
      "Saving test data\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def sequencer(df, one_hot_cols=None, include_transp=True):\n",
    "    sequences = []\n",
    "    maxLen = 0\n",
    "    # list the pieces\n",
    "    pieces = set(df.pieceId)\n",
    "    for p in pieces:\n",
    "        dp = df.loc[df.pieceId == p, :].copy()\n",
    "        transps = range(-3,4) if include_transp else [0]\n",
    "        for tr in transps:\n",
    "            d = dp.loc[dp.transposition == tr, :].copy()\n",
    "            maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "            d.drop(['pieceId', 'transposition'], axis=1, inplace=True)\n",
    "\n",
    "            # convert categories to one-hot\n",
    "            if one_hot_cols:\n",
    "                for attrib in one_hot_cols:\n",
    "                    d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "                    d.drop([attrib], axis=1, inplace=True)\n",
    "\n",
    "            # instance standardization for relevant features\n",
    "            feats = ['localTempo', 'peakLevel', 'ioiRatio']\n",
    "            aux = d.loc[:, feats]\n",
    "            moments = np.zeros((aux.shape[1], 2))\n",
    "            moments[:, 0] = aux.mean().to_numpy()\n",
    "            moments[:, 1] = aux.std().to_numpy()\n",
    "            d.loc[:, feats] = (aux - moments[:,0])/ moments[:,1]\n",
    "\n",
    "            # add <END> token to sequence\n",
    "            end = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "            end[\"pitch\"] = len(lex_to_ix) + 2\n",
    "            d = d.append(end)\n",
    "\n",
    "            # add <SOS> token to sequence\n",
    "            start = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "            start[\"pitch\"] = len(lex_to_ix) + 3\n",
    "            d = pd.concat([start, d])\n",
    "            \n",
    "            # separate output features\n",
    "            outCols = ['ioiRatio', 'timingDev', 'timingDevLocal', 'localTempo', 'peakLevel', 'startTime', 'durationSecs']\n",
    "            y = d.loc[:, outCols].copy()\n",
    "            d.drop(outCols, axis=1, inplace=True)\n",
    "\n",
    "            sequences.append(((d, y, moments), p, tr))\n",
    "    return sequences\n",
    "\n",
    "def standardize(df, moments=None, cols=None):\n",
    "    if cols is None:\n",
    "        cols = (df.dtypes == 'float64')\n",
    "    nums = df.loc[:,cols]\n",
    "    if moments is None:\n",
    "        moments = np.zeros((nums.shape[1],2))  # output mean and std for reverting predictions\n",
    "        moments[:,0] = nums.mean().to_numpy()\n",
    "        moments[:,1] = nums.std().to_numpy()\n",
    "    df.loc[:, cols] = (nums - moments[:,0]) / moments[:,1]\n",
    "    return moments, cols\n",
    "\n",
    "\n",
    "# Separate Training / Validation / Test:\n",
    "\n",
    "test = df.loc[df.pieceId.isin(test_pieces), :].copy()\n",
    "\n",
    "moments = None\n",
    "cols = None\n",
    "for i, (training_pieces, val_pieces) in enumerate(folds):\n",
    "    \n",
    "    train = df.loc[df.pieceId.isin(training_pieces), :].copy()\n",
    "    val = df.loc[df.pieceId.isin(val_pieces), :].copy()\n",
    "\n",
    "    # Standardization\n",
    "    moments, cols = standardize(train, cols=['beatDiff', 'duration', 'ioi', 'startTime', 'durationSecs', 'timingDev', 'timingDevLocal'])\n",
    "    standardize(val, moments=moments, cols=cols)\n",
    "    with open(os.path.join(dataFolder, 'LvB_' + IE + '_normalizer_fold_' + str(i) + '.data'), 'wb') as filehandle:\n",
    "        pickle.dump((moments, cols), filehandle)\n",
    "    \n",
    "    if IE == 'I':\n",
    "        train_seq = sequencer(train, one_hot_cols=['instrument'])\n",
    "        val_seq = sequencer(val, one_hot_cols=['instrument'], include_transp=False)\n",
    "    else:\n",
    "        train_seq = sequencer(train, one_hot_cols=['instrument', 'metricStrength'])\n",
    "        val_seq = sequencer(val, one_hot_cols=['instrument', 'metricStrength'], include_transp=False)\n",
    "\n",
    "    #  Save arrays\n",
    "    print('Saving fold ' + str(i))\n",
    "    with open(os.path.join(dataFolder, 'LvB_' + IE + '_train_sequences_fold_' + str(i) + '.data'), 'wb') as filehandle:\n",
    "        pickle.dump(train_seq, filehandle)\n",
    "    with open(os.path.join(dataFolder, 'LvB_' + IE + '_val_sequences_fold_' + str(i) + '.data'), 'wb') as filehandle:\n",
    "        pickle.dump(val_seq, filehandle)\n",
    "\n",
    "# Prepare test sequences\n",
    "print('Saving test data')\n",
    "standardize(test, moments=moments, cols=cols) # using last fold moments (it's good enough)\n",
    "if IE == 'I':\n",
    "    test_seq = sequencer(test, one_hot_cols=['instrument'], include_transp=False)\n",
    "else:\n",
    "    test_seq = sequencer(test, one_hot_cols=['instrument', 'metricStrength'], include_transp=False)\n",
    "with open(os.path.join(dataFolder, 'LvB_' + IE + '_test_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(test_seq, filehandle)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
