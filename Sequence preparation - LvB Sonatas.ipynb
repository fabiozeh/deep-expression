{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note-level dataset generation\n",
    "\n",
    "This notebook uses raw data from the MusicNet dataset to set up sequential numpy arrays suitable for training deep neural networks.\n",
    "\n",
    "**Before running:** Make sure to run the \"Levels Computation\" notebook to produce the numpy array files with global audio levels.\n",
    "\n",
    "If the intention is training a model in a remote server, instead of uploading the whole MusicNet dataset, the best strategy is to run the first two cells locally for generating the pandas csv, uploading that file and continuing from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START HERE ####\n",
    "\n",
    "dataFolder = 'data/'  # make sure the path to data folder is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing piece 21/21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import expression_modeling as m \n",
    "\n",
    "def preprocess(labelsDir, csv, outfile=None, include_header=False, include_transp=True):\n",
    "        \n",
    "    # load the symbolic information from the dataset\n",
    "    notearray = np.genfromtxt(os.path.join(labelsDir, csv + '.csv'), delimiter=',', names=True, dtype=['i', 'i', 'i', 'i', 'f', 'f', '|U40'])\n",
    "\n",
    "    # sort by score time first for correct parsing\n",
    "    notearray.sort(order=['start_beat', 'start_time'])\n",
    "\n",
    "    # load levels (generated by \"Levels computation\" notebook)\n",
    "    levels = np.load(os.path.join(dataFolder, 'levels', csv + '_global_lvls.npy'))\n",
    "\n",
    "    # piece key estimation (only major and minor for now)\n",
    "    isMajor, key, llhoodM, llhoodm = m.estimateKey(notearray['note'])\n",
    "    mode = m.Mode.major if isMajor else m.Mode.minor\n",
    "\n",
    "    # load time signature information\n",
    "    timesigdata = np.genfromtxt(os.path.join(dataFolder, 'musicnet', 'timesig.csv'), delimiter=',', names=True, dtype=['|U10', 'i', 'i', 'i'])\n",
    "    timesigdata = timesigdata[timesigdata['id'] == csv]\n",
    "    \n",
    "    piece = m.Piece(key=key, mode=mode, name=csv)\n",
    "    piece.beats_per_measure = timesigdata['upper'][0]\n",
    "    piece.time_sig_type = timesigdata['lower'][0]\n",
    "    piece.first_down_beat = timesigdata['pickup'][0]\n",
    "    piece.dynMean = np.mean(levels)\n",
    "    piece.dynStd = np.std(levels)\n",
    "    piece.startTime = notearray['start_time'][0]\n",
    "    piece.startBeat = notearray['start_beat'][0]\n",
    "    piece.endTime = notearray['end_time'][-1]\n",
    "    piece.endBeat = notearray['start_beat'][-1] + notearray['end_beat'][-1]\n",
    "    piece.part = m.buildPart(notearray, (levels - piece.dynMean)/piece.dynStd, 44100)\n",
    "\n",
    "    df = []\n",
    "    if include_transp:\n",
    "        for tr in range(-3,4):\n",
    "            di = m.buildNoteLevelDataframe(piece, transpose=tr)\n",
    "            di['transposition'] = tr\n",
    "            df.append(di)\n",
    "    else:\n",
    "        df = m.buildNoteLevelDataframe(piece, transpose=0)\n",
    "        df['transposition'] = 0\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    df['pieceId'] = int(csv)\n",
    "\n",
    "    if outfile is None:\n",
    "        outfile = open(os.path.join(dataFolder, csv + '.csv'), 'w+')\n",
    "        df.to_csv(outfile)\n",
    "    else:\n",
    "        df.to_csv(outfile, mode='a', header=include_header)\n",
    "    return outfile\n",
    "\n",
    "with open(os.path.join(dataFolder, 'musicnet', 'LvB_violinSonatas.json')) as f:\n",
    "    LvB = json.load(f)\n",
    "\n",
    "with open(os.path.join(dataFolder, 'LvB_violinSonatas.csv'), 'w+') as outfile:\n",
    "    mvt_list = []\n",
    "    header = True\n",
    "    for mvts in LvB.values():\n",
    "        mvt_list += mvts\n",
    "    for i,mvt in enumerate(mvt_list):\n",
    "        clear_output()\n",
    "        print(\"Processing piece \" + str(i+1) + '/' + str(len(mvt_list)))\n",
    "        preprocess(os.path.join(dataFolder, 'musicnet', 'train_labels'), str(mvt), outfile, include_header=header)\n",
    "        header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation (Features)\n",
    "\n",
    "The following cells use the CSV file produced above to format the data into sequences of notes containing a pitch vocabulary and a set of musicologically relevant features about the note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Note Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: 623623\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "with open(os.path.join(dataFolder, 'LvB_violinSonatas.csv'), 'r') as path:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "df.drop(['Unnamed: 0', 'harmony', 'bassNote'], axis=1, inplace=True)\n",
    "df['instrument'] = df['instrument'].astype(pd.CategoricalDtype([1, 41]))\n",
    "df['metricStrength'] = df['metricStrength'].astype(pd.CategoricalDtype([0, 1, 2, 3]))\n",
    "print('initial size: ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping pitch vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 81\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "voc = list(set(pitches))\n",
    "print('vocabulary size = ' + str(len(voc)))\n",
    "lex_to_ix = { lex:i+1 for i,lex in enumerate(voc) }  # index 0 is vacant for masking\n",
    "\n",
    "with open(os.path.join(dataFolder, 'LvB_pitch_dict.data'), 'wb') as filehandle:\n",
    "    pickle.dump(lex_to_ix, filehandle)\n",
    "\n",
    "pitches = list(df.loc[:,['pitch']].itertuples(index=False, name=None))\n",
    "df['pitch'] = [lex_to_ix.get(m, len(lex_to_ix)+1) for m in pitches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging data for sequential training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def sequencer(df, one_hot_cols=None, include_transp=True):\n",
    "    sequences = []\n",
    "    maxLen = 0\n",
    "    # list the pieces\n",
    "    pieces = set(df.pieceId)\n",
    "    for p in pieces:\n",
    "        dp = df.loc[df.pieceId == p, :].copy()\n",
    "        transps = range(-3,4) if include_transp else [0]\n",
    "        for tr in transps:\n",
    "            d = dp.loc[dp.transposition == tr, :].copy()\n",
    "            maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "            d.drop(['pieceId', 'transposition'], axis=1, inplace=True)\n",
    "\n",
    "            # convert categories to one-hot\n",
    "            if one_hot_cols:\n",
    "                for attrib in one_hot_cols:\n",
    "                    d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "                    d.drop([attrib], axis=1, inplace=True)\n",
    "\n",
    "            # instance standardization for relevant features\n",
    "            feats = ['localTempo', 'peakLevel', 'ioiRatio']\n",
    "            aux = d.loc[:, feats]\n",
    "            moments = np.zeros((aux.shape[1], 2))\n",
    "            moments[:, 0] = aux.mean().to_numpy()\n",
    "            moments[:, 1] = aux.std().to_numpy()\n",
    "            d.loc[:, feats] = (aux - moments[:,0])/ moments[:,1]\n",
    "\n",
    "            # add <END> token to sequence\n",
    "            end = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "            end[\"pitch\"] = len(lex_to_ix) + 2\n",
    "            d = d.append(end)\n",
    "\n",
    "            # add <SOS> token to sequence\n",
    "            start = pd.DataFrame(np.zeros((1,d.shape[1])), columns=d.columns)\n",
    "            start[\"pitch\"] = len(lex_to_ix) + 3\n",
    "            d = pd.concat([start, d])\n",
    "            \n",
    "            # separate output features\n",
    "            outCols = ['ioiRatio', 'timingDev', 'timingDevLocal', 'localTempo', 'peakLevel', 'startTime', 'durationSecs']\n",
    "            y = d.loc[:, outCols].copy()\n",
    "            d.drop(outCols, axis=1, inplace=True)\n",
    "\n",
    "            sequences.append(((d, y, moments), p, tr))\n",
    "    return sequences\n",
    "\n",
    "def standardize(df, moments=None, cols=None):\n",
    "    if cols is None:\n",
    "        cols = (df.dtypes == 'float64')\n",
    "    nums = df.loc[:,cols]\n",
    "    if moments is None:\n",
    "        moments = np.zeros((nums.shape[1],2))  # output mean and std for reverting predictions\n",
    "        moments[:,0] = nums.mean().to_numpy()\n",
    "        moments[:,1] = nums.std().to_numpy()\n",
    "    df.loc[:, cols] = (nums - moments[:,0]) / moments[:,1]\n",
    "    return moments, cols\n",
    "\n",
    "\n",
    "with open(os.path.join(dataFolder, 'musicnet', 'LvB_violinSonatas.json')) as f:\n",
    "    LvB = json.load(f)\n",
    "\n",
    "\n",
    "# Separate Training / Validation / Test:\n",
    "\n",
    "val_pieces = LvB[\"1\"]  # arbitrary choice\n",
    "test_pieces = LvB[\"2\"]  # arbitrary choice\n",
    "training_pieces = []\n",
    "for sonata in LvB.values():\n",
    "    for mvt in sonata:\n",
    "        if mvt not in val_pieces and mvt not in test_pieces:\n",
    "            training_pieces.append(mvt)\n",
    "\n",
    "train = df.loc[df.pieceId.isin(training_pieces), :].copy()\n",
    "val = df.loc[df.pieceId.isin(val_pieces), :].copy()\n",
    "test = df.loc[df.pieceId.isin(test_pieces), :].copy()\n",
    "\n",
    "\n",
    "# Prepare training sequences\n",
    "          \n",
    "moments, cols = standardize(train, cols=['beatDiff', 'duration', 'ioi', 'startTime', 'durationSecs', 'timingDev', 'timingDevLocal'])\n",
    "with open(os.path.join(dataFolder, 'LvB_normalizer.data'), 'wb') as filehandle:\n",
    "    pickle.dump((moments, cols), filehandle)\n",
    "train_seq = sequencer(train, one_hot_cols=['instrument'])\n",
    "\n",
    "\n",
    "# Prepare validation and test sequences\n",
    "          \n",
    "standardize(val, moments=moments, cols=cols)\n",
    "standardize(test, moments=moments, cols=cols)\n",
    "val_seq = sequencer(val, one_hot_cols=['instrument'], include_transp=False)\n",
    "test_seq = sequencer(test, one_hot_cols=['instrument'], include_transp=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#  Save arrays\n",
    "with open(os.path.join(dataFolder, 'LvB_train_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(train_seq, filehandle)\n",
    "with open(os.path.join(dataFolder, 'LvB_val_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(val_seq, filehandle)\n",
    "with open(os.path.join(dataFolder, 'LvB_test_sequences.data'), 'wb') as filehandle:\n",
    "    pickle.dump(test_seq, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
