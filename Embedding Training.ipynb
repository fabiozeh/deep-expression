{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "np.random.seed(1728)\n",
    "\n",
    "#  read dataset\n",
    "runLocal = True\n",
    "if runLocal:\n",
    "    pathRoot = 'data/'\n",
    "else:\n",
    "    pathRoot = '/content/drive/My Drive/colab_data/'\n",
    "\n",
    "with open(pathRoot + 'note_sequences_voc.data', 'rb') as seq_path:\n",
    "    sequences = pickle.load(seq_path)\n",
    "with open(pathRoot + 'note_sequences_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "    ix_to_lex = {v: k for k, v in lex_to_ix.items()}\n",
    "    vocab_size = len(lex_to_ix) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "def three_gram(vocab_size):\n",
    "    X = Input((1))\n",
    "    \n",
    "    #  Compute an embedding vector and apply it to all words in the bag X\n",
    "    emb = layers.Embedding(input_dim=vocab_size, output_dim=16, mask_zero=True)\n",
    "    emb_vec = emb(X)\n",
    "    h = layers.Dense(vocab_size, activation='tanh')(emb_vec)\n",
    "    p = layers.Softmax()(h)\n",
    "    \n",
    "    return Model(inputs=X, outputs=p), emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeGramGenerator(Sequence):\n",
    "    def __init__(self, data, batch_size=100,\n",
    "                 shuffle=True, fit=True, mini_batch_limit=np.inf):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.mini_batch_limit = mini_batch_limit\n",
    "        self.indexes = []\n",
    "        for si, (x, _) in enumerate(data):\n",
    "            for xi in range(x.shape[0]):\n",
    "                if xi > 3:\n",
    "                    self.indexes.append((si, xi, -3))\n",
    "                if xi > 2:\n",
    "                    self.indexes.append((si, xi, -2))\n",
    "                if xi > 1:\n",
    "                    self.indexes.append((si, xi, -1))\n",
    "                if xi + 1 < x.shape[0]:\n",
    "                    self.indexes.append((si, xi, 1))\n",
    "                if xi + 2 < x.shape[0]:\n",
    "                    self.indexes.append((si, xi, 2))\n",
    "                if xi + 3 < x.shape[0]:\n",
    "                    self.indexes.append((si, xi, 3))\n",
    "        np.random.shuffle(self.indexes)  # always shuffle once\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.min([len(self.indexes) / self.batch_size, self.mini_batch_limit]))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.batch_size\n",
    "        this_size = self.batch_size if index + self.batch_size < len(self.indexes) else len(self.indexes) - index\n",
    "        X = np.zeros((this_size, 1))\n",
    "        Y = np.zeros((this_size, 1, vocab_size))\n",
    "        for i in range(this_size):\n",
    "            X[i,:], Y[i,0,:] = self.__getsingleitem(index + i)\n",
    "        return X, Y\n",
    "    \n",
    "    def __getsingleitem(self, index):\n",
    "        (seq, stride, target) = self.indexes[index]\n",
    "        (N, _) = self.data[seq]\n",
    "        N = N.melody.to_numpy()\n",
    "        Y = np.zeros((1,vocab_size))\n",
    "        Y[0,N[stride + target]] = 1\n",
    "        return N[stride], Y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "np.random.shuffle(sequences)  # shuffle before splitting validation set\n",
    "# small ds for testing\n",
    "# sequences = sequences[:10]\n",
    "val_split_ix = int(0.9*len(sequences))\n",
    "generator = ThreeGramGenerator(sequences[:val_split_ix])\n",
    "val_gen = ThreeGramGenerator(sequences[val_split_ix:])\n",
    "\n",
    "model, embedding = three_gram(vocab_size)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(generator, epochs=4, validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
