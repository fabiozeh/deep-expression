{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note-level dataset generation\n",
    "\n",
    "This notebook uses raw data from the MusicNet dataset to set up sequential numpy arrays suitable for training deep neural networks.\n",
    "\n",
    "**Before running:** Make sure to run the \"Levels Computation\" notebook to produce the numpy array files with global audio levels.\n",
    "\n",
    "If the intention is training a model in a remote server, instead of uploading the whole MusicNet dataset, the best strategy is to run the first cell locally for generating the pandas csv, uploading that file and continuing from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START HERE ####\n",
    "\n",
    "dataFolder = 'data/'  # make sure the path to data folder is correct\n",
    "is_training_set = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin processing test set\n",
      "processing piece 10/10\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import expression_modeling as m \n",
    "\n",
    "def preprocess(labelsDir, instruments={41, 42, 43, 72, 74}, csvname='per_note_train', outfile=None):\n",
    "\n",
    "    dataset = [csv for csv in os.listdir(labelsDir)]\n",
    "    for i, csv in enumerate(dataset):\n",
    "        print('processing piece ' + str(i+1) + '/' + str(len(dataset)), end='\\r')\n",
    "        \n",
    "        # load the symbolic information from the dataset\n",
    "        notearray = np.genfromtxt(os.path.join(labelsDir, csv), delimiter=',', names=True, dtype=['i', 'i', 'i', 'i', 'f', 'f', '|U40'])\n",
    "\n",
    "        #  check if piece contains any desired instrument\n",
    "        csv_instruments = set(notearray['instrument'])\n",
    "        csv_desired_instruments = csv_instruments.intersection(instruments)\n",
    "        if not csv_desired_instruments:\n",
    "            continue\n",
    "        \n",
    "        #  load levels (generated by \"Levels computation\" notebook)\n",
    "        levels = np.load(dataFolder + 'levels/' + csv.replace('.csv', '_global_lvls.npy'))\n",
    "\n",
    "        # piece key estimation (only major and minor for now)\n",
    "        isMajor, key, llhoodM, llhoodm = m.estimateKey(notearray['note'])\n",
    "        mode = m.Mode.major if isMajor else m.Mode.minor\n",
    "\n",
    "        piece = m.Piece(key=key, mode=mode, name=csv)\n",
    "        piece.dynMean = np.mean(levels)\n",
    "        piece.dynStd = np.std(levels)\n",
    "        piece.startTime = notearray['start_time'][0]\n",
    "        piece.startBeat = notearray['start_beat'][0]\n",
    "        piece.endTime = notearray['end_time'][-1]\n",
    "        piece.endBeat = notearray['start_beat'][-1] + notearray['end_beat'][-1]\n",
    "        piece.parts = m.buildNoteParts(notearray, (levels - piece.dynMean)/piece.dynStd, 44100, csv_desired_instruments)\n",
    "        \n",
    "        df = []\n",
    "        for inst in csv_desired_instruments:\n",
    "            if is_training_set:\n",
    "                for tr in range(-3,4):\n",
    "                    di = m.buildNoteLevelDataframe(piece, inst, transpose=tr)\n",
    "                    di['transposition'] = tr\n",
    "                    df.append(di)\n",
    "            else:\n",
    "                di = m.buildNoteLevelDataframe(piece, inst, transpose=0)\n",
    "                di['transposition'] = 0\n",
    "                df.append(di)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "        df['pieceId'] = int(csv[0:-4])\n",
    "        \n",
    "        if outfile is None:\n",
    "            outfile = open(dataFolder + csvname + '.csv', 'w+')\n",
    "            df.to_csv(outfile)\n",
    "        else:\n",
    "            df.to_csv(outfile, mode='a', header=False)\n",
    "    return outfile\n",
    "\n",
    "clear_output()\n",
    "if is_training_set:\n",
    "    print('Begin training set')\n",
    "    f = preprocess('./data/musicnet/train_labels')\n",
    "    f.close()\n",
    "else:\n",
    "    print('Begin processing test set')\n",
    "    f = preprocess('./data/musicnet/test_labels', csvname='per_note_test')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: 2385495\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1728)\n",
    "\n",
    "#  read csv\n",
    "if is_training_set:\n",
    "    with open(dataFolder + 'per_note_train.csv', 'r') as path:\n",
    "        df = pd.read_csv(path)\n",
    "else:\n",
    "    with open(dataFolder + 'per_note_test.csv', 'r') as path:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df['bassNote'] = df['bassNote'].astype(pd.CategoricalDtype(list(range(0, 12))))\n",
    "df['metricStrength'] = df['metricStrength'].astype(pd.CategoricalDtype(list(range(0, 4))))\n",
    "df['instrument'] = df['instrument'].astype(pd.CategoricalDtype([41, 42, 43, 72, 74]))\n",
    "print('initial size: ' + str(len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping melody vocabulary.\n",
      "vocabulary size = 830\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if is_training_set:\n",
    "    #  generate vocabulary of (pitch, bassNote)\n",
    "\n",
    "    print(\"Mapping melody vocabulary.\")\n",
    "    melodies = list(df.loc[:,['pitch', 'bassNote']].itertuples(index=False, name=None))\n",
    "    voc = list(set(melodies))\n",
    "    print('vocabulary size = ' + str(len(voc)))\n",
    "    lex_to_ix = { lex:i+1 for i,lex in enumerate(voc) } # index 0 is vacant for masking\n",
    "\n",
    "    with open(dataFolder + 'note_sequences_dict.data', 'wb') as filehandle:\n",
    "        pickle.dump(lex_to_ix, filehandle)\n",
    "else:\n",
    "    with open(dataFolder + 'note_sequences_dict.data', 'rb') as filehandle:\n",
    "        lex_to_ix = pickle.load(filehandle)\n",
    "\n",
    "melodies = list(df.loc[:,['pitch', 'bassNote']].itertuples(index=False, name=None))\n",
    "df['melody'] = [lex_to_ix.get(m, len(lex_to_ix)+1) for m in melodies]\n",
    "df.drop(['pitch', 'bassNote'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pieces: 167\n"
     ]
    }
   ],
   "source": [
    "def sequencer(df, one_hot_cols=None):\n",
    "    sequences = []\n",
    "    maxLen = 0\n",
    "    #  list the pieces\n",
    "    pieces = set(df.pieceId)\n",
    "    for p in pieces:\n",
    "        # list the instruments\n",
    "        piece_seq = []\n",
    "        dp = df.loc[df.pieceId == p, :]\n",
    "        instruments = set(dp.instrument)\n",
    "        for i in instruments:\n",
    "            di = dp.loc[dp.instrument == i, :]\n",
    "            for tr in range(-3,4):\n",
    "                d = di.loc[di.transposition == tr, :]\n",
    "                maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "                d.drop(['pieceId', 'transposition', 'startTime', 'durationSecs'], axis=1, inplace=True)\n",
    "                outCols = ['timingDev', 'timingDevLocal', 'localTempo', 'peakLevel']\n",
    "                #  convert categories to one-hot\n",
    "                if one_hot_cols:\n",
    "                    for attrib in one_hot_cols:\n",
    "                        d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "                        d.drop([attrib], axis=1, inplace=True)\n",
    "                y = d.loc[:, outCols]\n",
    "                d.drop(outCols, axis=1, inplace=True)\n",
    "                piece_seq.append((d, y, tr, i))\n",
    "        sequences.append((piece_seq, p))\n",
    "    return sequences\n",
    "\n",
    "def standardize(df, moments=None, cols=None):\n",
    "    if cols is None:\n",
    "        cols = (df.dtypes == 'float64')\n",
    "    nums = df.loc[:,cols]\n",
    "    if moments is None:\n",
    "        moments = np.zeros((nums.shape[1],2)) # output mean and std for reverting predictions\n",
    "        moments[:,0] = nums.mean().to_numpy()\n",
    "        moments[:,1] = nums.std().to_numpy()\n",
    "    df.loc[:, cols] = (nums - moments[:,0]) / moments[:,1]\n",
    "    return moments, cols\n",
    "\n",
    "if is_training_set:\n",
    "    moments, cols = standardize(df)\n",
    "    with open(dataFolder + 'normalizer.data', 'wb') as filehandle:\n",
    "        pickle.dump((moments, cols), filehandle)\n",
    "else:\n",
    "    with open(dataFolder + 'normalizer.data', 'rb') as filehandle:\n",
    "        moments, cols = pickle.load(filehandle)\n",
    "    standardize(df, moments=moments, cols=cols)\n",
    "\n",
    "sequences = sequencer(df, one_hot_cols=['metricStrength', 'instrument'])\n",
    "\n",
    "print(\"Number of pieces: \" + str(len(sequences)))\n",
    "\n",
    "#  check for NaNs\n",
    "# nans = np.argwhere(np.isnan(X))\n",
    "\n",
    "#  eliminate NaNs\n",
    "# okrows = np.logical_not(np.logical_or(np.isnan(X).any(axis=(1,2)), np.isnan(Y).any(axis=(1,2))))\n",
    "# X = X[okrows,:,:]\n",
    "# Y = Y[okrows,:,:]\n",
    "# moments = moments[okrows,:,:]\n",
    "# pd_idx = pd_idx[okrows,:]\n",
    "# print(\"dataset size without NaN: \" + str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#  Save arrays\n",
    "if is_training_set:\n",
    "    with open(dataFolder + 'note_sequences.data', 'wb') as filehandle:\n",
    "        pickle.dump(sequences, filehandle)\n",
    "else:\n",
    "    with open(dataFolder + 'note_sequences_test.data', 'wb') as filehandle:\n",
    "        pickle.dump(sequences, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
