{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note-level dataset generation\n",
    "\n",
    "This notebook uses raw data from the MusicNet dataset to set up sequential numpy arrays suitable for training deep neural networks.\n",
    "\n",
    "**Before running:** Make sure to run the \"Levels Computation\" notebook to produce the numpy array files with global audio levels.\n",
    "\n",
    "If the intention is training a model in a remote server, instead of uploading the whole MusicNet dataset, the best strategy is to run the first cell locally for generating the pandas csv, uploading that file and continuing from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import expression_modeling as m\n",
    "\n",
    "def preprocess(labelsDir, instruments={41, 42, 43, 72, 74}, csvname='per_note', outfile=None):\n",
    "\n",
    "    dataset = [csv for csv in os.listdir(labelsDir)]\n",
    "    for i, csv in enumerate(dataset):\n",
    "        print('processing piece ' + str(i+1) + '/' + str(len(dataset)), end='\\r')\n",
    "        \n",
    "        # load the symbolic information from the dataset\n",
    "        notearray = np.genfromtxt(os.path.join(labelsDir, csv), delimiter=',', names=True, dtype=['i', 'i', 'i', 'i', 'f', 'f', '|U40'])\n",
    "\n",
    "        #  check if piece contains any desired instrument\n",
    "        csv_instruments = set(notearray['instrument'])\n",
    "        csv_desired_instruments = csv_instruments.intersection(instruments)\n",
    "        if not csv_desired_instruments:\n",
    "            continue\n",
    "        \n",
    "        #  load levels (generated by \"Levels computation\" notebook)\n",
    "        levels = np.load('data/levels/' + csv.replace('.csv', '_global_lvls.npy'))\n",
    "\n",
    "        # piece key estimation (only major and minor for now)\n",
    "        isMajor, key, llhoodM, llhoodm = m.estimateKey(notearray['note'])\n",
    "        mode = m.Mode.major if isMajor else m.Mode.minor\n",
    "\n",
    "        piece = m.Piece(key=key, mode=mode, name=csv)\n",
    "        piece.dynMean = np.mean(levels)\n",
    "        piece.dynStd = np.std(levels)\n",
    "        piece.startTime = notearray['start_time'][0]\n",
    "        piece.startBeat = notearray['start_beat'][0]\n",
    "        piece.endTime = notearray['end_time'][-1]\n",
    "        piece.endBeat = notearray['start_beat'][-1] + notearray['end_beat'][-1]\n",
    "        piece.parts = m.buildNoteParts(notearray, (levels - piece.dynMean)/piece.dynStd, 44100, csv_desired_instruments)\n",
    "        \n",
    "        df = []\n",
    "        for inst in csv_desired_instruments:\n",
    "            di = m.buildNoteLevelDataframe(piece, inst)\n",
    "            df.append(di)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "        df['pieceId'] = int(csv[0:-4])\n",
    "        \n",
    "        if outfile is None:\n",
    "            outfile = open('data/' + csvname + '.csv', 'w+')\n",
    "            df.to_csv(outfile)\n",
    "        else:\n",
    "            df.to_csv(outfile, mode='a', header=False)\n",
    "    return outfile\n",
    "\n",
    "# clear_output()\n",
    "# print('Begin training set')\n",
    "# f = preprocess('./data/musicnet/train_labels')\n",
    "# f.close()\n",
    "\n",
    "print('Begin processing test set')\n",
    "f = preprocess('./data/musicnet/test_labels', csvname='per_note_test')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1728)\n",
    "\n",
    "#  read csv\n",
    "path = open('data/per_note_test.csv', 'r')\n",
    "df = pd.read_csv(path)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df['pitch'] = df['pitch'].astype(pd.CategoricalDtype(list(range(36, 109))))\n",
    "df['bassNote'] = df['bassNote'].astype(pd.CategoricalDtype(list(range(0, 12))))\n",
    "df['metricStrength'] = df['metricStrength'].astype(pd.CategoricalDtype(list(range(0, 4))))\n",
    "df['instrument'] = df['instrument'].astype(pd.CategoricalDtype([41, 42, 43, 72, 74]))\n",
    "print('initial size: ' + str(len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#  generate vocabulary of (pitch, bassNote)\n",
    "\n",
    "print(\"Mapping melody vocabulary.\")\n",
    "melodies = list(df.loc[:,['pitch', 'bassNote']].itertuples(index=False, name=None))\n",
    "voc = list(set(melodies))\n",
    "print('vocabulary size = ' + str(len(voc)))\n",
    "lex_to_ix = { lex:i+1 for i,lex in enumerate(voc) } # index 0 is vacant for masking\n",
    "\n",
    "with open('data/note_sequences_dict.data', 'wb') as filehandle:\n",
    "    pickle.dump(lex_to_ix, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/note_sequences_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "\n",
    "melodies = list(df.loc[:,['pitch', 'bassNote']].itertuples(index=False, name=None))\n",
    "df['melody'] = [lex_to_ix.get(m, len(lex_to_ix)+1) for m in melodies]\n",
    "df.drop(['pitch', 'bassNote'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(df, as_ndarray=False, one_hot_cols=None):\n",
    "  sequences = []\n",
    "  maxLen = 0\n",
    "  #  list the instruments\n",
    "  instruments = set(df.instrument)\n",
    "  for ins in instruments:\n",
    "    # list the pieces\n",
    "    di = df.loc[df.instrument == ins, :]\n",
    "    pieces = set(di.pieceId)\n",
    "    for p in pieces:\n",
    "      d = di.loc[di.pieceId == p, :]\n",
    "      maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "      d.drop(['pieceId', 'startTime', 'durationSecs'], axis=1, inplace=True)\n",
    "      outCols = ['timingDev', 'timingDevLocal', 'localTempo', 'peakLevel']\n",
    "      #  convert categories to one-hot\n",
    "      if one_hot_cols:\n",
    "            for attrib in one_hot_cols:\n",
    "              d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "              d.drop([attrib], axis=1, inplace=True)\n",
    "      y = d.loc[:, outCols]\n",
    "      d.drop(outCols, axis=1, inplace=True)\n",
    "      sequences.append((d, y))\n",
    "  if as_ndarray:\n",
    "    X = np.full((len(sequences), maxLen, len(sequences[0][0].columns)), 0, dtype='float64')\n",
    "    Y = np.full((len(sequences), maxLen, len(sequences[0][1].columns)), 0, dtype='float64')\n",
    "    pd_idx = np.full((len(sequences), maxLen), -1e4, dtype='int32')\n",
    "    for i, s in enumerate(sequences):\n",
    "      (x, y) = s\n",
    "      X[i, 0:len(x), :] = x\n",
    "      Y[i, 0:len(y), :] = y\n",
    "      pd_idx[i, 0:len(x)] = x.index\n",
    "    return X, Y, pd_idx\n",
    "  else:\n",
    "    return sequences\n",
    "\n",
    "def standardize(df, moments=None, cols=None):\n",
    "    if cols is None:\n",
    "        cols = (df.dtypes == 'float64')\n",
    "    nums = df.loc[:,cols]\n",
    "    if moments is None:\n",
    "        moments = np.zeros((nums.shape[1],2)) # output mean and std for reverting predictions\n",
    "        moments[:,0] = nums.mean().to_numpy()\n",
    "        moments[:,1] = nums.std().to_numpy()\n",
    "    df.loc[:, cols] = (nums - moments[:,0]) / moments[:,1]\n",
    "    return moments, cols\n",
    "\n",
    "is_train = False\n",
    "\n",
    "if is_train:\n",
    "    moments, cols = standardize(df)\n",
    "    with open('data/normalizer.data', 'wb') as filehandle:\n",
    "        pickle.dump((moments, cols), filehandle)\n",
    "else:\n",
    "    with open('data/normalizer.data', 'rb') as filehandle:\n",
    "        moments, cols = pickle.load(filehandle)\n",
    "    standardize(df, moments=moments, cols=cols)\n",
    "\n",
    "sequences = sequencer(df, one_hot_cols=['metricStrength', 'instrument'])\n",
    "\n",
    "print(\"Number of pieces: \" + str(len(sequences)))\n",
    "\n",
    "#  check for NaNs\n",
    "# nans = np.argwhere(np.isnan(X))\n",
    "\n",
    "#  eliminate NaNs\n",
    "# okrows = np.logical_not(np.logical_or(np.isnan(X).any(axis=(1,2)), np.isnan(Y).any(axis=(1,2))))\n",
    "# X = X[okrows,:,:]\n",
    "# Y = Y[okrows,:,:]\n",
    "# moments = moments[okrows,:,:]\n",
    "# pd_idx = pd_idx[okrows,:]\n",
    "# print(\"dataset size without NaN: \" + str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#  Save arrays\n",
    "# np.save('data/X_sequential_per_note.npy', X)\n",
    "# np.save('data/Y_sequential_per_note.npy', Y)\n",
    "# np.save('data/Y_moments.npy', moments)\n",
    "# np.save('data/dataframe_idx.npy', pd_idx)\n",
    "\n",
    "with open('data/note_sequences_voc_test.data', 'wb') as filehandle:\n",
    "    pickle.dump(sequences, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
