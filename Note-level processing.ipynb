{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin processing test set\n",
      "processing piece 10/10\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import expression_modeling as m\n",
    "\n",
    "def preprocess(labelsDir, instruments={41, 42, 43, 72, 74}, csvname='per_note', outfile=None):\n",
    "\n",
    "    dataset = [csv for csv in os.listdir(labelsDir)]\n",
    "    for i, csv in enumerate(dataset):\n",
    "        print('processing piece ' + str(i+1) + '/' + str(len(dataset)), end='\\r')\n",
    "        \n",
    "        # load the symbolic information from the dataset\n",
    "        notearray = np.genfromtxt(os.path.join(labelsDir, csv), delimiter=',', names=True, dtype=['i', 'i', 'i', 'i', 'f', 'f', '|U40'])\n",
    "\n",
    "        #  check if piece contains any desired instrument\n",
    "        csv_instruments = set(notearray['instrument'])\n",
    "        csv_desired_instruments = csv_instruments.intersection(instruments)\n",
    "        if not csv_desired_instruments:\n",
    "            continue\n",
    "        \n",
    "        #  load levels (generated by \"Levels computation\" notebook)\n",
    "        levels = np.load('data/levels/' + csv.replace('.csv', '_global_lvls.npy'))\n",
    "\n",
    "        # piece key estimation (only major and minor for now)\n",
    "        isMajor, key, llhoodM, llhoodm = m.estimateKey(notearray['note'])\n",
    "        mode = m.Mode.major if isMajor else m.Mode.minor\n",
    "\n",
    "        piece = m.Piece(key=key, mode=mode, name=csv)\n",
    "        piece.dynMean = np.mean(levels)\n",
    "        piece.dynStd = np.std(levels)\n",
    "        piece.startTime = notearray['start_time'][0]\n",
    "        piece.startBeat = notearray['start_beat'][0]\n",
    "        piece.endTime = notearray['end_time'][-1]\n",
    "        piece.endBeat = notearray['start_beat'][-1] + notearray['end_beat'][-1]\n",
    "        piece.parts = m.buildNoteParts(notearray, (levels - piece.dynMean)/piece.dynStd, 44100, csv_desired_instruments)\n",
    "        \n",
    "        df = []\n",
    "        for inst in csv_desired_instruments:\n",
    "            di = m.buildNoteLevelDataframe(piece, inst)\n",
    "            df.append(di)\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "        df['pieceId'] = int(csv[0:-4])\n",
    "        \n",
    "        if outfile is None:\n",
    "            outfile = open('data/' + csvname + '.csv', 'w+')\n",
    "            df.to_csv(outfile)\n",
    "        else:\n",
    "            df.to_csv(outfile, mode='a', header=False)\n",
    "    return outfile\n",
    "\n",
    "# clear_output()\n",
    "# print('Begin training set')\n",
    "# f = preprocess('./data/musicnet/train_labels')\n",
    "# f.close()\n",
    "\n",
    "print('Begin processing test set')\n",
    "f = preprocess('./data/musicnet/test_labels', csvname='per_note_test')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size: 340785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1728)\n",
    "\n",
    "#  read csv\n",
    "path = open('data/per_note_train.csv', 'r')\n",
    "df = pd.read_csv(path)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df['pitch'] = df['pitch'].astype(pd.CategoricalDtype(list(range(36, 109))))\n",
    "df['bassNote'] = df['bassNote'].astype(pd.CategoricalDtype(list(range(0, 12))))\n",
    "df['metricStrength'] = df['metricStrength'].astype(pd.CategoricalDtype(list(range(0, 4))))\n",
    "df['instrument'] = df['instrument'].astype(pd.CategoricalDtype([41, 42, 43, 72, 74]))\n",
    "print('initial size: ' + str(len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (357, 2459, 104)\n"
     ]
    }
   ],
   "source": [
    "def sequencer(df):\n",
    "  sequences = []\n",
    "  maxLen = 0\n",
    "  #  list the instruments\n",
    "  instruments = set(df.instrument)\n",
    "  for ins in instruments:\n",
    "    # list the pieces\n",
    "    di = df.loc[df.instrument == ins, :]\n",
    "    pieces = set(di.pieceId)\n",
    "    for p in pieces:\n",
    "      d = di.loc[di.pieceId == p, :]\n",
    "      maxLen = len(d) if len(d) > maxLen else maxLen\n",
    "      d = d.drop(['pieceId', 'startTime', 'durationSecs', 'peakLevel'], axis=1)\n",
    "      outCols = ['timingDev', 'timingDevLocal', 'localTempo']\n",
    "      \n",
    "      #  standardize features\n",
    "      moments = np.zeros((len(outCols),2)) # output mean and std for reverting predictions\n",
    "      outs = d.loc[:,outCols]\n",
    "      moments[:,0] = outs.mean().to_numpy()\n",
    "      moments[:,1] = outs.std().to_numpy()\n",
    "      nums = d.loc[:,(d.dtypes == 'float64')]\n",
    "      d.loc[:, (d.dtypes == 'float64')] = (nums - nums.mean()) / nums.std()\n",
    "      #  convert categories to one-hot\n",
    "      for attrib in ['metricStrength', 'pitch', 'bassNote', 'instrument']:\n",
    "          d = pd.concat([d, pd.get_dummies(d[attrib], prefix=attrib)], axis=1)\n",
    "          d.drop([attrib], axis=1, inplace=True)\n",
    "      \n",
    "      y = d.loc[:, outCols]\n",
    "      d.drop(outCols, axis=1, inplace=True)\n",
    "      sequences.append((moments, d, y))\n",
    "  X = np.full((len(sequences), maxLen, len(sequences[0][1].columns)), 0, dtype='float64')\n",
    "  Y = np.full((len(sequences), maxLen, len(sequences[0][2].columns)), 0, dtype='float64')\n",
    "  moments = np.zeros((len(sequences), len(outCols), 2))\n",
    "  pd_idx = np.full((len(sequences), maxLen), -1e4, dtype='int32')\n",
    "  for i, s in enumerate(sequences):\n",
    "      (mm, x, y) = s\n",
    "      X[i, 0:len(x), :] = x\n",
    "      Y[i, 0:len(y), :] = y\n",
    "      moments[i, :, :] = mm\n",
    "      pd_idx[i, 0:len(x)] = x.index\n",
    "  return X, Y, moments, pd_idx\n",
    "        \n",
    "#  make data sequential \n",
    "X, Y, moments, pd_idx = sequencer(df)\n",
    "print(\"dataset size: \" + str(X.shape))\n",
    "\n",
    "#  check for NaNs\n",
    "# nans = np.argwhere(np.isnan(X))\n",
    "\n",
    "#  eliminate NaNs\n",
    "# okrows = np.logical_not(np.logical_or(np.isnan(X).any(axis=(1,2)), np.isnan(Y).any(axis=(1,2))))\n",
    "# X = X[okrows,:,:]\n",
    "# Y = Y[okrows,:,:]\n",
    "# moments = moments[okrows,:,:]\n",
    "# pd_idx = pd_idx[okrows,:]\n",
    "# print(\"dataset size without NaN: \" + str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save arrays\n",
    "np.save('data/X_sequential_per_note.npy', X)\n",
    "np.save('data/Y_sequential_per_note.npy', Y)\n",
    "np.save('data/Y_moments.npy', moments)\n",
    "np.save('data/dataframe_idx.npy', pd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
