{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs2Yk5y3IxKb"
   },
   "source": [
    "Deep artificial neural network for expressive timing and dynamics predictions in musical pieces\n",
    "---------------\n",
    "\n",
    "This notebook loads a sequential dataset with score and performance information and uses it to train and test a deep artificial neural network for generating onset timing deviation and peak loudness level of notes from musical pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing to install XLA (for training on TPUs) and pytorch-lightning (skip if not using Google Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "\n",
    "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install pytorch_lightning --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters to set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runLocal = True  # set to False for using Google Colab\n",
    "\n",
    "output_cols = [\"peakLevel\"]\n",
    "DEV_RUN = False\n",
    "SCHEDULER_STEP_SIZE = 4\n",
    "SCHEDULER_GAMMA = 0.25\n",
    "LR = 1e-6\n",
    "SEQ_LEN = 200\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.1\n",
    "EVAL_STRIDE = 160 #int(SEQ_LEN / 2)  # score notes sliding window\n",
    "EVAL_CTX = 20 #int(EVAL_STRIDE / 2)  # no. of note predictions to ignore in sequence start\n",
    "PAD_END = True\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 8\n",
    "STATE_DICT_NAME = 'hpc_logs/version_2137133/2021-03-06-hp200-128-lvl.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting path and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "if runLocal:\n",
    "    pathRoot = 'data/'\n",
    "else:\n",
    "    pathRoot = '/content/drive/My Drive/colab_data/'\n",
    "    !wget \"https://raw.githubusercontent.com/fabiozeh/deep-expression/master/dataloader.py\"\n",
    "\n",
    "    \n",
    "with open(os.path.join(pathRoot, 'LvB_train_sequences.data'), 'rb') as seq_path:\n",
    "    train = pickle.load(seq_path)\n",
    "with open(os.path.join(pathRoot, 'LvB_val_sequences.data'), 'rb') as seq_path:\n",
    "    val = pickle.load(seq_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzL3Y9MOIxLG"
   },
   "source": [
    "#### Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(1728)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_x, vocab_size, embed_size, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.pitchEmbedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=0)\n",
    "        self.harmonyRhythmProjector = nn.Linear(in_features=n_x - 1, out_features=embed_size)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(2*embed_size)\n",
    "        self.rnn = nn.GRU(2 * embed_size, 2 * embed_size, num_layers=1, bidirectional=True)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(4*embed_size)\n",
    "    \n",
    "    def forward(self, pitch, score_feats, lengths):\n",
    "        \n",
    "        pitch = self.pitchEmbedding(pitch)\n",
    "        score_feats = self.harmonyRhythmProjector(score_feats)\n",
    "        src_vec = torch.cat([pitch, score_feats], dim=2)\n",
    "        src_vec = self.norm1(self.drop1(src_vec))\n",
    "        \n",
    "        sequence = nn.utils.rnn.pack_padded_sequence(src_vec, lengths.cpu(), enforce_sorted=False)\n",
    "        output, _ = self.rnn(sequence)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        return src_vec, self.norm2(self.drop2(output))\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_y, hidden_size, enc_hidden_size, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.y_proj = nn.Linear(n_y, hidden_size)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, 1, kdim=enc_hidden_size, vdim=enc_hidden_size)\n",
    "        self.rnn = nn.GRU(2*hidden_size, hidden_size, num_layers=1)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.ff1 = nn.Linear(4*hidden_size, 2*hidden_size)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        self.ff2 = nn.Linear(2*hidden_size, n_y, bias=False)\n",
    "        \n",
    "    def forward(self, x_vec, y_prev, encoder_out, dec_hidden):\n",
    "        \"\"\"\n",
    "        Generates outputs for a single step in the sequence (one note)\n",
    "        \"\"\"\n",
    "        y_projected = self.drop1(self.y_proj(y_prev))  # (1, batch_size, hidden_size)\n",
    "        \n",
    "        # shapes: (1, b, h) <- ( (1, b, h), (len_seq, b, enc_hidden), (len_seq, b, enc_hidden) )\n",
    "        context, _ = self.attention(dec_hidden, encoder_out, encoder_out)\n",
    "        rnn_out, new_dec_hidden = self.rnn(torch.cat([y_projected, context], dim=2), dec_hidden)\n",
    "        \n",
    "        rnn_out = self.norm(self.drop2(rnn_out))\n",
    "        out = self.ff2(self.drop3(F.relu(self.ff1(torch.cat([rnn_out, y_projected, context, x_vec], dim=2)))))\n",
    "        return out, new_dec_hidden\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_x, n_y, vocab_size, hidden_size=64, dropout_rate=0.1, lr=1e-4, context=0, window=0):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        assert hidden_size % 2 == 0, \"hidden_size must be multiple of 2\"\n",
    "        \n",
    "        self.n_y = n_y\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.window = window\n",
    "        self.context = context\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "        self.encoder = Encoder(n_x, vocab_size, int(hidden_size/2), dropout_rate)\n",
    "        self.decoder = Decoder(self.n_y, hidden_size, 2*hidden_size, dropout_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, pitch, score_feats, lengths):\n",
    "        \"\"\"\n",
    "        Generate the entire sequence \n",
    "        \"\"\"\n",
    "        src_vec, encoded_score = self.encoder(pitch, score_feats, lengths)\n",
    "        hidden = torch.zeros((1, pitch.shape[1], self.hidden_size), device=self.device)\n",
    "        y = torch.zeros((pitch.shape[0], pitch.shape[1], self.n_y), device=self.device)\n",
    "        prev_y = torch.zeros((1, pitch.shape[1], self.n_y), device=self.device)\n",
    "        for i in range(pitch.shape[0]):\n",
    "            prev_y, hidden = self.decoder(src_vec[i,:,:].unsqueeze(0), prev_y, encoded_score, hidden)\n",
    "            y[i,:,:] = prev_y\n",
    "        return y\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        This method doesn't use self.forward directly so we can apply teacher forcing\n",
    "        on a fraction of the steps.\n",
    "        \"\"\"\n",
    "        pitch, score_feats, y, lengths = batch\n",
    "        if len(pitch.shape) < 2:\n",
    "            pitch = pitch.unsqueeze(1)\n",
    "            score_feats = score_feats.unsqueeze(1)\n",
    "            y = y.unsqueeze(1)\n",
    "\n",
    "        # encode x (score)\n",
    "        src_vec, encoded_score = self.encoder(pitch, score_feats, lengths)\n",
    "        \n",
    "        # iterate generating y\n",
    "        teacher_forcing_ratio = 0.5\n",
    "        \n",
    "        hidden = torch.zeros((1, score_feats.shape[1], self.hidden_size), device=self.device)\n",
    "        y_hat = torch.zeros((y.shape[0], y.shape[1], self.n_y), device=self.device)\n",
    "        prev_y = torch.zeros((1, score_feats.shape[1], self.n_y), device=self.device)\n",
    "        for i in range(pitch.shape[0]):\n",
    "            prev_y, hidden = self.decoder(src_vec[i,:,:].unsqueeze(0), prev_y, encoded_score, hidden)\n",
    "            y_hat[i,:,:] = prev_y\n",
    "            if self.rng.random() > teacher_forcing_ratio:\n",
    "                prev_y = y[i,:,:].view(1, -1, self.n_y)\n",
    "        \n",
    "        if self.window:\n",
    "            ctx = self.context\n",
    "            if not ctx:\n",
    "                ctx = y_hat.shape[0] - self.window\n",
    "            y_hat = y_hat[ctx:ctx + self.window, :, :]\n",
    "            y = y[ctx:ctx + self.window, :, :]\n",
    "\n",
    "        loss =  F.mse_loss(y_hat, y)\n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pitch, score_feats, y, lengths = batch\n",
    "        if len(pitch.shape) < 2:\n",
    "            pitch = pitch.unsqueeze(1)\n",
    "            score_feats = score_feats.unsqueeze(1)\n",
    "            y = y.unsqueeze(1)\n",
    "\n",
    "        y_hat = self.forward(pitch, score_feats, lengths)\n",
    "        \n",
    "        if self.window:\n",
    "            ctx = self.context\n",
    "            if not ctx:\n",
    "                ctx = y_hat.shape[0] - self.window\n",
    "            y_hat = y_hat[ctx:ctx + self.window, :, :]\n",
    "            y = y[ctx:ctx + self.window, :, :]\n",
    "            \n",
    "        return {'val_loss': F.mse_loss(y_hat, y)}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathRoot + 'LvB_pitch_dict.data', 'rb') as filehandle:\n",
    "    lex_to_ix = pickle.load(filehandle)\n",
    "    ix_to_lex = {v: k for k, v in lex_to_ix.items()}\n",
    "with open(pathRoot + 'LvB_normalizer.data', 'rb') as filehandle:\n",
    "    moments, cols = pickle.load(filehandle)\n",
    "    moments = dict(zip(cols, list(moments)))\n",
    "with open(os.path.join(pathRoot, 'LvB_test_sequences.data'), 'rb') as seq_path:\n",
    "    test = pickle.load(seq_path)\n",
    "\n",
    "model = Net(test[0][0][0].shape[1],\n",
    "            len(output_cols),\n",
    "            vocab_size=len(ix_to_lex) + 4,  # 0 = pad, len+1 = UKN, len+2 = END, len+3 = SOS\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            dropout_rate=DROPOUT,\n",
    "            lr=LR,\n",
    "            context=(EVAL_CTX if PAD_END else 0),\n",
    "            window=(EVAL_STRIDE if PAD_END else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB1zXiP9IxLV"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "JpghmUE3IxLX",
    "outputId": "6c2e7186-6bf7-41ac-ea28-387da0e9d393"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import dataloader as dl\n",
    "\n",
    "if runLocal:\n",
    "    trainer = pl.Trainer(max_epochs=NUM_EPOCHS, fast_dev_run=DEV_RUN, val_check_interval=0.25)\n",
    "    workers = 4\n",
    "else:\n",
    "    trainer = pl.Trainer(gpus=1, accelerator='dp', fast_dev_run=DEV_RUN,\n",
    "                         progress_bar_refresh_rate=20, max_epochs=NUM_EPOCHS,\n",
    "                         val_check_interval=0.25)\n",
    "    workers = 0\n",
    "\n",
    "if SEQ_LEN == 0:\n",
    "    train_ds = dl.FullPieceDataset(train, \n",
    "                                   vocab_col=test[0][0][0].columns.get_loc(\"pitch\"),\n",
    "                                   output_cols=output_cols)\n",
    "    val_ds = dl.FullPieceDataset(val, \n",
    "                                 vocab_col=test[0][0][0].columns.get_loc(\"pitch\"),\n",
    "                                 output_cols=output_cols)\n",
    "else:\n",
    "    train_ds = dl.TrainDataset(train, \n",
    "                               vocab_col=test[0][0][0].columns.get_loc(\"pitch\"),\n",
    "                               sequence_length=SEQ_LEN,\n",
    "                               output_cols=output_cols,\n",
    "                               context=EVAL_CTX,\n",
    "                               dummy=DEV_RUN)\n",
    "    val_ds = dl.ValidationDataset(val, \n",
    "                                  vocab_col=test[0][0][0].columns.get_loc(\"pitch\"),\n",
    "                                  sequence_length=SEQ_LEN,\n",
    "                                  output_cols=output_cols,\n",
    "                                  stride=EVAL_STRIDE,\n",
    "                                  context=EVAL_CTX,\n",
    "                                  pad_both_ends=PAD_END,\n",
    "                                  device=model.device)\n",
    "trainer.fit(model, \n",
    "            DataLoader(train_ds,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       num_workers = workers,\n",
    "                       shuffle=True,\n",
    "                       collate_fn=dl.TrainDataset.collate_fn),\n",
    "            DataLoader(val_ds,\n",
    "                       batch_size=None,\n",
    "                       num_workers=workers))\n",
    "\n",
    "#  Save model\n",
    "torch.save(model.state_dict(), pathRoot + STATE_DICT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), pathRoot + STATE_DICT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mv-wgSVTIxLj"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2eV2YV0IxL1",
    "outputId": "2af81d2a-bd9b-47a8-a0d3-038baaf6e948"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import dataloader as dl\n",
    "\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load(STATE_DICT_NAME))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "#  Compute note-level error\n",
    "\n",
    "def evaluation(sequences, sequence_length, model, stride=0, context=0):\n",
    "    # context = no. of note predictions to ignore in sequence start\n",
    "    loader = dl.ValidationDataset(sequences,\n",
    "                                  vocab_col=sequences[0][0][0].columns.get_loc(\"pitch\"),\n",
    "                                  sequence_length=sequence_length,\n",
    "                                  output_cols=output_cols,\n",
    "                                  stride=stride,\n",
    "                                  context=context,\n",
    "                                  pad_both_ends=PAD_END,\n",
    "                                  device=model.device)\n",
    "    Y_hat = []\n",
    "    for piece in range(len(loader)):\n",
    "        (pch, s_f, Y, lth) = loader[piece]\n",
    "        out = model(pch, s_f, lth)\n",
    "        out = out.detach().numpy()\n",
    "        y_hat_p = np.zeros((sequences[piece][0][1].shape[0], len(output_cols)))\n",
    "        ind = 0\n",
    "        for s in range(out.shape[1] - 1):\n",
    "            y_hat_p[ind:ind + stride, :] = out[context:context + stride, s, :]\n",
    "            ind += stride\n",
    "        y_hat_p[ind:, :] = out[context:context + y_hat_p.shape[0] - ind, -1, :]\n",
    "        Y_hat.append(y_hat_p)\n",
    "    return Y_hat\n",
    "\n",
    "Yhat = evaluation(val, SEQ_LEN, model, stride=EVAL_STRIDE, context=EVAL_CTX)\n",
    "mse = np.zeros((len(val), Yhat[0].shape[1]))\n",
    "ms = np.zeros((len(val), Yhat[0].shape[1]))\n",
    "for i, S in enumerate(val):\n",
    "    Y = S[0][1]\n",
    "    Y = Y.loc[:,output_cols]\n",
    "    mse[i,:] = np.mean((Yhat[i][:Y.shape[0],:] - Y) ** 2)\n",
    "    ms[i,:] = np.mean(Y ** 2)\n",
    "    \n",
    "print('Validation set MSE for y_0: ' + str(np.mean(mse[:,0])) + '     mean square val: ' + str(np.mean(ms[:,0])))\n",
    "print('Minimum y_0 MSE among pieces: ' + str(mse[:,0].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "piece = 0\n",
    "attr = ['peakLevel']\n",
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(Yhat[piece][:200,0])\n",
    "plt.plot(test[piece][0][1].loc[:,attr].to_numpy()[:200])\n",
    "# print(test_sequences[piece][1].columns[attr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of dynamics of different performances of same piece for context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(train[35][0][1].loc[:,attr].to_numpy()[2500:])\n",
    "plt.plot(train[70][0][1].loc[:,attr].to_numpy()[2500:])\n",
    "plt.figure(figsize=(21, 5))\n",
    "plt.plot(train[42][0][1].loc[:,attr].to_numpy()[1000:1300])\n",
    "plt.plot(train[77][0][1].loc[:,attr].to_numpy()[1000:1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_human1 = np.mean((train[70][0][1].loc[:, 'peakLevel'].iloc[:3120].to_numpy('float64') - train[35][0][1].loc[:, 'peakLevel'].iloc[:3120].to_numpy('float64')) ** 2)\n",
    "mse_human2 = np.mean((train[42][0][1].loc[:, 'peakLevel'].to_numpy('float64') - train[77][0][1].loc[:, 'peakLevel'].to_numpy('float64')) ** 2)\n",
    "\n",
    "print(\"MSE between two performances of sonata 7, 2nd mvmt.: \" + str(mse_human1))\n",
    "print(\"MSE between two performances of sonata 7, 3rd mvmt.: \" + str(mse_human2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listen to a piece synthesized with the generated expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import IPython.display\n",
    "import expression_modeling as m\n",
    "\n",
    "# piece to synthesize:\n",
    "pieceNum = 0\n",
    "pieceId = val[pieceNum][1]\n",
    "print(pieceId)\n",
    "\n",
    "pred = Yhat[pieceNum][:,0]\n",
    "ref = val[pieceNum][0][1].ioiRatio\n",
    "no_dev = np.asarray([val[pieceNum][0][2][2,0]] * val[pieceNum][0][1].shape[0])\n",
    "dev_rand = np.random.normal(size=val[pieceNum][0][1].shape[0]) * val[pieceNum][0][2][2,1] + val[pieceNum][0][2][2,0]\n",
    "\n",
    "pm = m.midi_performance(val[pieceNum][0], pred, moments, ix_to_lex, method='ioiRatio')\n",
    "IPython.display.Audio(pm.fluidsynth(fs=44100), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PhraseDynamicsLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
